{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "continent-spiritual",
   "metadata": {},
   "source": [
    "\n",
    "# Recurrent Neural Networks with Top-k Gains for Session-based Recommendations\n",
    "\n",
    "\n",
    "Avec l'avènement des plateformes de streaming et de commerce en ligne les systèmes de recommandation ont pris une place incontournable dans notre vie de chaque jour. Les systèmes de recommandation sont aujourd'hui incontournables lors de nos visites quotidiennes sur le web. les systèmes de recommandation sont des algorithmes dont le but est de faire des suggestions pertinentes à l'usager. par exemple lorsque vous allez sur l'application netflix, vous avez des suggestions de films qui pourraient vous intéresser. ces algorithmes sont technologies qui rendent les entreprises extrêmement concurrentielles. Il existe deux grandes familles de méthodes pour construire un système de recommandation:\n",
    "- les méthodes par filtrage collaboratif;\n",
    "- les méthodes basées sur le contenu.\n",
    "Les méthodes collaboratives pour les systèmes de recommandation sont des méthodes qui se basent uniquement sur les interactions passées enregistrées entre les utilisateurs et les éléments afin de produire de nouvelles recommandations. Ces interactions sont stockées dans la \"matrice des interactions entre utilisateurs et articles\". L'idée principale qui régit les méthodes collaboratives est que ces interactions passées entre utilisateurs et articles sont suffisantes pour détecter des utilisateurs similaires et/ou des articles similaires et faire des prédictions basées sur ces proximités estimées. Contrairement aux méthodes de collaboration qui reposent uniquement sur les interactions entre l'utilisateur et l'article, les approches basées sur le contenu utilisent des informations supplémentaires sur les utilisateurs et/ou les articles. L'idée derrière ces méthodes est de construire un modèle, basé sur les \"caractéristiques\" disponibles, qui expliquent les interactions observées entre l'utilisateur et les articles. par exemple si nous voulons faire des recommandations sur un site de vente en ligne nous pouvons ajouter des variables comme le sexe du visiteur, son age, sa profession... \n",
    "\n",
    "Cependant ces deux familles ont une principale limite: elles sont incapable de faire une recommandation lorsqu'il s'agit d'un nouvel utilisateur. Un utilisateur qui n'a pas d'historique. Ceci est le but de l'article étudié pour de projet, construire un système de recommandation qui soit capable de faire des recommandation lorsque l'utilisateur n'a pas d'historique sur le site. une solution triviale à ce problème est de faire le item-to-item approche. Avec cette approche on recommande à l'utilisateur des articles qui sont similaires. Dans cet article, l'auteur n'utilisera pas cette approche mais plutôt des réseaux de neuronnes, ici, des réseaux de neuronnes récurrents. Ces derniers sont réputés pour leur excellente habilité à modéliser des données séquentielles. Avec ces réseau l'auteur va modéliser toute la session de l'utilisateur afin de pouvoir faire des prédictions.\n",
    "\n",
    "## présentation des travaux précédents\n",
    "Pleines de solutions avaient été proposés pour ce problème notemment celle avec la matrice des articles similaires, et des méthodes avec Reccurent Neural Networks(RNN), les LSTMs et les GRU. Dans cette section nous nous focaliseront sur l'article de référence de l'auteur. Nous présenteront la méthode de résoltion dans cet article afin de présenter plus les améliorations de cette dernière dans l'article que nous étudions. L'article sur lequel l'auteur a bati son raisonnement est **SESSION-BASED RECOMMENDATIONS WITH RECURRENT NEURAL NETWORKS**, disponible ici [ici](https://arxiv.org/abs/1511.06939). \n",
    "\n",
    "Dans cet article la version de RNN utilisée est celle de General Recurent Unit (GRU), cette version permet de résoudre le problème du gradient qui disparait avec les RNN. le modèle utilisé ici est constitué d'une couche d'embedding, des couches de GRU, des feedforwards layers et la sortie du réseau contient les différents scores des articles prédisant ainsi le prochain article sur lequel l'utilisateur cliquera. L'entrée du réseau est l'état actuel de la session. Ci-dessous une représentation du réseau.\n",
    "<img src=\"images/architecture.png\"/>\n",
    "\n",
    "Pour l'entrainement du réseau les auteurs considèrent des mini-batches de sessions parrallèles. En effet les RNN sont toujours entrainé sur des batches de données, et la taille des données d'entrées doit être fixe. ceci ne peut être obtenu avec ce type de données car les sessions n'ont pas les mêmes durée, de plus vu qu'on veut modéliser la session entière ça ne fait pas sens de couper une session pour en faire un batch. pour résoudre ce problème les auteurs utlisent à la fois plusieurs sessions d'utilisateurs et forment des batchs avec avec les éléments des différentes sessions (pour cela ils supposent l'indépendance entre les sessions). ci dessous une illustration de la formation des batches.\n",
    "<img src=\"images/batchs.png\"/>\n",
    "Ensuite pour chaque session il faudra prédire les prochaines sélections de l'utilisateur. Le problème ici est qu'un site peut contenir des milliers d'articles et que ceux qui intéressent vraiment sont ceux qui pourront intéresser l'utilisateur. Si on considèrent tous les articles cela conduirait à un vecteur sparse car les articles jugés (par le réseau) non intéressant pas l'utilisateur auront des probabilités très faibles. Ainsi pour résoudre ce problème avec la sparsité du résultat les auteurs adoptent cette méthode d'échantillonnage des articles. Ils considèrent tous les autres articles du mini batchs comme des exemples négatifs.\n",
    "\n",
    "Pour faire la backpropagation avec leur réseau, les auteurs considèrent deux fonctions de perte:\n",
    "- BPR: Bayesian Personalized Ranking. c'est une méthode de factorisation matricielle qui utilise la perte de classement par paire. Il compare le score d'un positif et d'un négatif échantillonné point. Ici on compare le score de l'élément positif avec plusieurs éléments échantillonnés et utilisons leur moyenne comme la perte. Ainsi on compare le score de l'élément positif avec celui des négatifs. la formule de cette perte est donnée ci-dessous:\n",
    "    $$ L_s^{BPR} = \\frac{-1}{N_s} \\sum_{j=1}^{N_s}{log(\\sigma(\\hat r_{s,i} - \\hat r_{s,j}))} $$\n",
    "où {N_s} est la taille de l'échantillon, {\\hat r_{s,k}} est le score de l'article k, i est le prochain item (celui qu'on cherche à prédire) et j les échantillons négatifs.\n",
    "\n",
    "- TOP1: cette perte a été conçue par les auteurs pour cette tâche, elle régularise l'approximation du rang relatif de l'article concerné. Cette perte est donnée par la formule suivante:\n",
    "$$ L_s^{TOP1} = \\frac{-1}{N_s} \\sum_{j=1}^{N_s}{\\sigma(\\hat r_{s,j} - \\hat r_{s,i}) + \\sigma(\\hat r_{s,j}^2)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "formal-belgium",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'GRU4Rec'...\n"
     ]
    }
   ],
   "source": [
    "##clonage du github où se trouve l'implémentation de l'article\n",
    "!git clone https://github.com/hidasib/GRU4Rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lightweight-circuit",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: theano in c:\\users\\mbial\\anaconda3\\lib\\site-packages (1.0.5)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\mbial\\anaconda3\\lib\\site-packages (from theano) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\mbial\\anaconda3\\lib\\site-packages (from theano) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\mbial\\anaconda3\\lib\\site-packages (from theano) (1.19.2)\n"
     ]
    }
   ],
   "source": [
    "## installation des requirements\n",
    "!pip install theano \n",
    "conda install -c conda-forge pygpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-polyester",
   "metadata": {},
   "source": [
    "### importation des librairies utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import lib\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-unemployment",
   "metadata": {},
   "source": [
    "## Nétoyage des données\n",
    "Dans cette section il est question de nétoyer les données et de les séparer en train, validation, test. Pour cela Nous lisons le fichier contenant toutes les données. Ce fichier contient trois colonnes:\n",
    "- l'identifiant de la session;\n",
    "- le temps;\n",
    "- les identifiants des items sur lesquels l'utilisateur a cliqué pendant la session.\n",
    "\n",
    "De prime abord nous retenons les sessions de longueur au moins égale à deux ensuite les items qui ont été sélectionnés au moins 5 fois puis une selection des sessions au moins égales à 2. Pour séparer le dataset en train et en test il faut éviter de couper une session (cf article) et donc la stratégie utilisée est de mettre dans le fichier train les sessions qui ont été effectuées jusqu'à un moment donné (tmax-86400) et le reste dans le dataset test. La même stratégie est appliquée sur le dataset train pour le séparer en train et en validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "coupled-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "### liens pour les données\n",
    "##le premier permet de récupérer les données à nétoyer et le second lien va contenir les données nétoyées\n",
    "PATH_TO_ORIGINAL_DATA = 'C:/Users/mbial/OneDrive/Bureau/2020_2021/2020-2021/ENSAE/projet/datasets/RSC15/'\n",
    "PATH_TO_PROCESSED_DATA = 'C:/Users/mbial/OneDrive/Bureau/2020_2021/2020-2021/ENSAE/projet/datasets/RSC15/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "previous-revelation",
   "metadata": {},
   "outputs": [],
   "source": [
    "##reading of the entire dataset\n",
    "data = pd.read_csv('yoochoose-clicks.dat', sep=',', header=None, usecols=[0,1,2], dtype={0:np.int32, 1:str, 2:np.int64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "minimal-attitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['SessionId', 'TimeStr', 'ItemId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beneficial-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Time'] = data.TimeStr.apply(lambda x: dt.datetime.strptime(x, '%Y-%m-%dT%H:%M:%S.%fZ').timestamp()) #This is not UTC. It does not really matter.\n",
    "del(data['TimeStr']) ##delete the column TimeStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "controversial-duration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33003944, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape ##Check the shape of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sorted-match",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionId</th>\n",
       "      <th>ItemId</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>214536502</td>\n",
       "      <td>1.396861e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>214536500</td>\n",
       "      <td>1.396861e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>214536506</td>\n",
       "      <td>1.396861e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>214577561</td>\n",
       "      <td>1.396861e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>214662742</td>\n",
       "      <td>1.396872e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SessionId     ItemId          Time\n",
       "0          1  214536502  1.396861e+09\n",
       "1          1  214536500  1.396861e+09\n",
       "2          1  214536506  1.396861e+09\n",
       "3          1  214577561  1.396861e+09\n",
       "4          2  214662742  1.396872e+09"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "divided-salmon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SessionId\n",
      "1           4\n",
      "2           6\n",
      "3           3\n",
      "4           2\n",
      "6           2\n",
      "           ..\n",
      "11562156    2\n",
      "11562157    2\n",
      "11562158    3\n",
      "11562159    1\n",
      "11562161    1\n",
      "Length: 9249729, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "session_lengths = data.groupby('SessionId').size() #size of each session\n",
    "print(session_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "brief-bulgarian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31744233, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Only keep session with length >1\n",
    "data = data[np.in1d(data.SessionId, session_lengths[session_lengths>1].index)]\n",
    "data.shape #check the new shape of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "signal-authentication",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ItemId\n",
       "214507224     32\n",
       "214507226     13\n",
       "214507228      1\n",
       "214507239      6\n",
       "214507256      1\n",
       "              ..\n",
       "1178835219     1\n",
       "1178835247     1\n",
       "1178835585     1\n",
       "1178835641     1\n",
       "1178837797    12\n",
       "Length: 52069, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_supports = data.groupby('ItemId').size() #number of time each item was selected\n",
    "item_supports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "mechanical-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Only keep items selected at least 5 times\n",
    "data = data[np.in1d(data.ItemId, item_supports[item_supports>=5].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "critical-thought",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31713448, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape #check new data shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "described-silence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check session length\n",
    "session_lengths = data.groupby('SessionId').size()\n",
    "np.min(session_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "impressed-potter",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Only keep session with length >1\n",
    "data = data[np.in1d(data.SessionId, session_lengths[session_lengths>=2].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "variable-catholic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1412038799.43"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmax = data.Time.max() #check the max time of sessions\n",
    "tmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fourth-jonathan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SessionId\n",
       "1           1.396861e+09\n",
       "2           1.396872e+09\n",
       "3           1.396438e+09\n",
       "4           1.396866e+09\n",
       "6           1.396797e+09\n",
       "                ...     \n",
       "11562152    1.411718e+09\n",
       "11562153    1.411571e+09\n",
       "11562156    1.411700e+09\n",
       "11562157    1.411641e+09\n",
       "11562158    1.411701e+09\n",
       "Name: Time, Length: 7981581, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_max_times = data.groupby('SessionId').Time.max()\n",
    "session_max_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "strategic-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_train = session_max_times[session_max_times < tmax-86400].index ##selected index for train data\n",
    "session_test = session_max_times[session_max_times >= tmax-86400].index ##selected index for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "integrated-assurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[np.in1d(data.SessionId, session_train)] ##train data\n",
    "test = data[np.in1d(data.SessionId, session_test)]\n",
    "test = test[np.in1d(test.ItemId, train.ItemId)] ##test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "regional-sperm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionId</th>\n",
       "      <th>ItemId</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32186847</th>\n",
       "      <td>11265009</td>\n",
       "      <td>214586805</td>\n",
       "      <td>1.411997e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32186848</th>\n",
       "      <td>11265009</td>\n",
       "      <td>214509260</td>\n",
       "      <td>1.411997e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32186868</th>\n",
       "      <td>11265017</td>\n",
       "      <td>214857547</td>\n",
       "      <td>1.412011e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32186869</th>\n",
       "      <td>11265017</td>\n",
       "      <td>214857268</td>\n",
       "      <td>1.412011e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32186870</th>\n",
       "      <td>11265017</td>\n",
       "      <td>214857260</td>\n",
       "      <td>1.412011e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003915</th>\n",
       "      <td>11299816</td>\n",
       "      <td>214859859</td>\n",
       "      <td>1.412014e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003916</th>\n",
       "      <td>11299816</td>\n",
       "      <td>214859859</td>\n",
       "      <td>1.412014e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003917</th>\n",
       "      <td>11299816</td>\n",
       "      <td>214859859</td>\n",
       "      <td>1.412014e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003918</th>\n",
       "      <td>11299816</td>\n",
       "      <td>214746399</td>\n",
       "      <td>1.412015e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003919</th>\n",
       "      <td>11299816</td>\n",
       "      <td>214567327</td>\n",
       "      <td>1.412016e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71222 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          SessionId     ItemId          Time\n",
       "32186847   11265009  214586805  1.411997e+09\n",
       "32186848   11265009  214509260  1.411997e+09\n",
       "32186868   11265017  214857547  1.412011e+09\n",
       "32186869   11265017  214857268  1.412011e+09\n",
       "32186870   11265017  214857260  1.412011e+09\n",
       "...             ...        ...           ...\n",
       "33003915   11299816  214859859  1.412014e+09\n",
       "33003916   11299816  214859859  1.412014e+09\n",
       "33003917   11299816  214859859  1.412014e+09\n",
       "33003918   11299816  214746399  1.412015e+09\n",
       "33003919   11299816  214567327  1.412016e+09\n",
       "\n",
       "[71222 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fuzzy-horizon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionId</th>\n",
       "      <th>ItemId</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>214536502</td>\n",
       "      <td>1.396861e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>214536500</td>\n",
       "      <td>1.396861e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>214536506</td>\n",
       "      <td>1.396861e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>214577561</td>\n",
       "      <td>1.396861e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>214662742</td>\n",
       "      <td>1.396872e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003939</th>\n",
       "      <td>11299809</td>\n",
       "      <td>214819412</td>\n",
       "      <td>1.411630e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003940</th>\n",
       "      <td>11299809</td>\n",
       "      <td>214830939</td>\n",
       "      <td>1.411631e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003941</th>\n",
       "      <td>11299811</td>\n",
       "      <td>214854855</td>\n",
       "      <td>1.411578e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003942</th>\n",
       "      <td>11299811</td>\n",
       "      <td>214854838</td>\n",
       "      <td>1.411578e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003943</th>\n",
       "      <td>11299811</td>\n",
       "      <td>214848658</td>\n",
       "      <td>1.411578e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31637239 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          SessionId     ItemId          Time\n",
       "0                 1  214536502  1.396861e+09\n",
       "1                 1  214536500  1.396861e+09\n",
       "2                 1  214536506  1.396861e+09\n",
       "3                 1  214577561  1.396861e+09\n",
       "4                 2  214662742  1.396872e+09\n",
       "...             ...        ...           ...\n",
       "33003939   11299809  214819412  1.411630e+09\n",
       "33003940   11299809  214830939  1.411631e+09\n",
       "33003941   11299811  214854855  1.411578e+09\n",
       "33003942   11299811  214854838  1.411578e+09\n",
       "33003943   11299811  214848658  1.411578e+09\n",
       "\n",
       "[31637239 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "organic-solution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "tslength = test.groupby('SessionId').size()\n",
    "print(np.min(tslength))\n",
    "test = test[np.in1d(test.SessionId, tslength[tslength>=2].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "prospective-project",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full train set\n",
      "\tEvents: 31637239\n",
      "\tSessions: 7966257\n",
      "\tItems: 37483\n",
      "Test set\n",
      "\tEvents: 71222\n",
      "\tSessions: 15324\n",
      "\tItems: 6751\n"
     ]
    }
   ],
   "source": [
    "##save train data and test data\n",
    "print('Full train set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}'.format(len(train), train.SessionId.nunique(), train.ItemId.nunique()))\n",
    "train.to_csv(PATH_TO_PROCESSED_DATA + 'rsc15_train_full.txt', sep='\\t', index=False)\n",
    "print('Test set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}'.format(len(test), test.SessionId.nunique(), test.ItemId.nunique()))\n",
    "test.to_csv(PATH_TO_PROCESSED_DATA + 'rsc15_test.txt', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "considered-glance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "\tEvents: 31579006\n",
      "\tSessions: 7953885\n",
      "\tItems: 37483\n",
      "Validation set\n",
      "\tEvents: 58233\n",
      "\tSessions: 12372\n",
      "\tItems: 6359\n"
     ]
    }
   ],
   "source": [
    "##same strategy for the selection of validation data\n",
    "##split the previous train data on train data and validation data\n",
    "##save dataframe\n",
    "tmax = train.Time.max()\n",
    "session_max_times = train.groupby('SessionId').Time.max()\n",
    "session_train = session_max_times[session_max_times < tmax-86400].index\n",
    "session_valid = session_max_times[session_max_times >= tmax-86400].index\n",
    "train_tr = train[np.in1d(train.SessionId, session_train)]\n",
    "valid = train[np.in1d(train.SessionId, session_valid)]\n",
    "valid = valid[np.in1d(valid.ItemId, train_tr.ItemId)]\n",
    "tslength = valid.groupby('SessionId').size()\n",
    "valid = valid[np.in1d(valid.SessionId, tslength[tslength>=2].index)]\n",
    "print('Train set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}'.format(len(train_tr), train_tr.SessionId.nunique(), train_tr.ItemId.nunique()))\n",
    "train_tr.to_csv(PATH_TO_PROCESSED_DATA + 'rsc15_train_tr.txt', sep='\\t', index=False)\n",
    "print('Validation set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}'.format(len(valid), valid.SessionId.nunique(), valid.ItemId.nunique()))\n",
    "valid.to_csv(PATH_TO_PROCESSED_DATA + 'rsc15_train_valid.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-brand",
   "metadata": {},
   "source": [
    "### Expérimentations\n",
    "\n",
    "#### prise en main des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "egyptian-requirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    \"\"\"\n",
    "    Read data,\n",
    "    create indices for items ID\n",
    "    created sorted or no indices for SessionID\n",
    "    \n",
    "    arguments:\n",
    "    ----------\n",
    "    path: The link to find data,\n",
    "    session_key: The name of session ID on dataset\n",
    "    item_key: The name of Item ID on dataset\n",
    "    time_key: The name of Time on dataset\n",
    "    n_sample : Number of sample should be used on dataset if positive\n",
    "    itemmap : list of item indices (none per default)\n",
    "    time_sort: wheter the session ID index should be sorted by time or not\n",
    "    \"\"\"\n",
    "    def __init__(self, path, sep='\\t', session_key='SessionId', item_key='ItemId', time_key='Time', n_sample=-1, itemmap=None, itemstamp=None, time_sort=False):\n",
    "        \n",
    "        self.df = pd.read_csv(path, sep=sep, dtype={session_key: int, item_key: int, time_key: float}) #rezd csv\n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.time_key = time_key\n",
    "        self.time_sort = time_sort\n",
    "        if n_sample > 0:\n",
    "            self.df = self.df[:n_sample]\n",
    "\n",
    "        \n",
    "        self.add_item_indices(itemmap=itemmap) # Add colummn item index to data\n",
    "        \"\"\"\n",
    "        Sort the df by time, and then by session ID. That is, df is sorted by session ID and\n",
    "        clicks within a session are next to each other, where the clicks within a session are time-ordered.\n",
    "        \"\"\"\n",
    "        self.df.sort_values([session_key, time_key], inplace=True)\n",
    "        self.click_offsets = self.get_click_offset()\n",
    "        self.session_idx_arr = self.order_session_idx()\n",
    "\n",
    "    def add_item_indices(self, itemmap=None):\n",
    "        \"\"\"\n",
    "        Add item index column named \"item_idx\" to the df\n",
    "        Args:\n",
    "            itemmap (pd.DataFrame): mapping between the item Ids and indices\n",
    "        \"\"\"\n",
    "        if itemmap is None:\n",
    "            item_ids = self.df[self.item_key].unique()  # numpy ND_array with all item_key\n",
    "            item2idx = pd.Series(data=np.arange(len(item_ids)),\n",
    "                                 index=item_ids) # Numpy array with index of each item_key\n",
    "            # Build itemmap is a DataFrame that have 2 columns (self.item_key, 'item_idx)\n",
    "            itemmap = pd.DataFrame({self.item_key: item_ids,\n",
    "                                   'item_idx': item2idx[item_ids].values})\n",
    "        self.itemmap = itemmap\n",
    "        self.df = pd.merge(self.df, self.itemmap, on=self.item_key, how='inner')\n",
    "\n",
    "    def get_click_offset(self):\n",
    "        \"\"\"return a cumulative sum of sessions' size\n",
    "        \n",
    "        \"\"\"\n",
    "        offsets = np.zeros(self.df[self.session_key].nunique() + 1, dtype=np.int32)\n",
    "        offsets[1:] = self.df.groupby(self.session_key).size().cumsum()\n",
    "        return offsets\n",
    "\n",
    "    def order_session_idx(self):\n",
    "        \"\"\"Return sorted indices by time of session key if mentionned (self.time_sort=True)\n",
    "        else return indices of session key\n",
    "        \"\"\"\n",
    "        if self.time_sort:\n",
    "            sessions_start_time = self.df.groupby(self.session_key)[self.time_key].min().values ##minimum time of each session\n",
    "            session_idx_arr = np.argsort(sessions_start_time) #return indice of session_key that would sort sessions_start_time\n",
    "        else:\n",
    "            session_idx_arr = np.arange(self.df[self.session_key].nunique())\n",
    "        return session_idx_arr\n",
    "\n",
    "    @property\n",
    "    def items(self):\n",
    "        \"\"\"number of unique items on session\n",
    "        \"\"\"\n",
    "        return self.itemmap[self.item_key].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "considerable-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, dataset, batch_size=50):\n",
    "        \"\"\"\n",
    "        A class for creating session-parallel mini-batches.\n",
    "\n",
    "        Args:\n",
    "             dataset (SessionDataset): the session dataset to generate the batches from\n",
    "             batch_size (int): size of the batch\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\" Returns the iterator for producing session-parallel training mini-batches.\n",
    "\n",
    "        Yields:\n",
    "            input (B,): torch.FloatTensor. Item indices that will be encoded as one-hot vectors later.\n",
    "            target (B,): a Variable that stores the target item indices\n",
    "            masks: Numpy array indicating the positions of the sessions to be terminated\n",
    "        \"\"\"\n",
    "        # initializations\n",
    "        df = self.dataset.df ##dataframe whith sessions, items and times\n",
    "        click_offsets = self.dataset.click_offsets #cumulative number of sessions clicks\n",
    "        session_idx_arr = self.dataset.session_idx_arr ##indices array of each session\n",
    "\n",
    "        iters = np.arange(self.batch_size) #iterations\n",
    "        maxiter = iters.max() ##maximum number of iterations\n",
    "        start = click_offsets[session_idx_arr[iters]] #The begin of the session in same batch on the dataset\n",
    "        end = click_offsets[session_idx_arr[iters] + 1] #The begin of the session out of the batch\n",
    "        mask = []  # indicator for the sessions to be terminated\n",
    "        finished = False\n",
    "\n",
    "        while not finished:\n",
    "            minlen = (end - start).min()\n",
    "            # Item indices(for embedding) for clicks where the first sessions start\n",
    "            idx_target = df.item_idx.values[start]\n",
    "\n",
    "            for i in range(minlen - 1):\n",
    "                # Build inputs & targets\n",
    "                idx_input = idx_target\n",
    "                idx_target = df.item_idx.values[start + i + 1]\n",
    "                inputs = torch.LongTensor(idx_input)\n",
    "                target = torch.LongTensor(idx_target)\n",
    "                yield inputs, target, mask\n",
    "\n",
    "            # click indices where a particular session meets second-to-last element\n",
    "            start = start + (minlen - 1)\n",
    "            # see if how many sessions should terminate\n",
    "            mask = np.arange(len(iters))[(end - start) <= 1]\n",
    "            for idx in mask:\n",
    "                maxiter += 1\n",
    "                if maxiter >= len(click_offsets) - 1:\n",
    "                    finished = True\n",
    "                    break\n",
    "                # update the next starting/ending point\n",
    "                iters[idx] = maxiter\n",
    "                start[idx] = click_offsets[session_idx_arr[maxiter]]\n",
    "                end[idx] = click_offsets[session_idx_arr[maxiter] + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-longer",
   "metadata": {},
   "source": [
    "#### Fonctions de pertes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "tight-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, loss_type='TOP1', use_cuda=False):\n",
    "        \"\"\" An abstract loss function that can supports custom loss functions compatible with PyTorch.\"\"\"\n",
    "        super(LossFunction, self).__init__()\n",
    "        self.loss_type = loss_type\n",
    "        self.use_cuda = use_cuda\n",
    "        if loss_type == 'CrossEntropy':\n",
    "            self._loss_fn = SampledCrossEntropyLoss(use_cuda)\n",
    "        elif loss_type == 'TOP1':\n",
    "            self._loss_fn = TOP1Loss()\n",
    "        elif loss_type == 'BPR':\n",
    "            self._loss_fn = BPRLoss()\n",
    "        elif loss_type == 'TOP1-max':\n",
    "            self._loss_fn = TOP1_max()\n",
    "        elif loss_type == 'BPR-max':\n",
    "            self._loss_fn = BPR_max()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, logit):\n",
    "        return self._loss_fn(logit)\n",
    "\n",
    "\n",
    "class SampledCrossEntropyLoss(nn.Module):\n",
    "    \"\"\" CrossEntropyLoss with n_classes = batch_size = the number of samples in the session-parallel mini-batch \"\"\"\n",
    "    def __init__(self, use_cuda):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "             use_cuda (bool): whether to use cuda or not\n",
    "        \"\"\"\n",
    "        super(SampledCrossEntropyLoss, self).__init__()\n",
    "        self.xe_loss = nn.CrossEntropyLoss()\n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "    def forward(self, logit):\n",
    "        batch_size = logit.size(1)\n",
    "        target = Variable(torch.arange(batch_size).long())\n",
    "        if self.use_cuda:\n",
    "            target = target.cuda()\n",
    "\n",
    "        return self.xe_loss(logit, target)\n",
    "\n",
    "\n",
    "class BPR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BPRLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logit):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logit (BxB): Variable that stores the logits for the items in the mini-batch\n",
    "                         The first dimension corresponds to the batches, and the second\n",
    "                         dimension corresponds to sampled number of items to evaluate\n",
    "        \"\"\"\n",
    "        # differences between the item scores\n",
    "        diff = logit.diag().view(-1, 1).expand_as(logit) - logit\n",
    "        # final loss\n",
    "        loss = -torch.mean(F.logsigmoid(diff))\n",
    "        return loss\n",
    "\n",
    "\n",
    "class BPR_max(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BPR_max, self).__init__()\n",
    "    def forward(self, logit):\n",
    "        logit_softmax = F.softmax(logit, dim=1)\n",
    "        diff = logit.diag().view(-1, 1).expand_as(logit) - logit\n",
    "        loss = -torch.log(torch.mean(logit_softmax * torch.sigmoid(diff)))\n",
    "        return loss\n",
    "\n",
    "\n",
    "class TOP1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TOP1Loss, self).__init__()\n",
    "    def forward(self, logit):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logit (BxB): Variable that stores the logits for the items in the mini-batch\n",
    "                         The first dimension corresponds to the batches, and the second\n",
    "                         dimension corresponds to sampled number of items to evaluate\n",
    "        \"\"\"\n",
    "        diff = -(logit.diag().view(-1, 1).expand_as(logit) - logit)\n",
    "        loss = torch.sigmoid(diff).mean() + torch.sigmoid(logit ** 2).mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class TOP1_max(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TOP1_max, self).__init__()\n",
    "\n",
    "    def forward(self, logit):\n",
    "        logit_softmax = F.softmax(logit, dim=1)\n",
    "        diff = -(logit.diag().view(-1, 1).expand_as(logit) - logit)\n",
    "        loss = torch.mean(logit_softmax * (torch.sigmoid(diff) + torch.sigmoid(logit ** 2)))\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-reputation",
   "metadata": {},
   "source": [
    "#### L'optimiseur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "satisfied-trader",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Optimizer:\n",
    "    def __init__(self, params, optimizer_type='Adagrad', lr=.05,\n",
    "                 momentum=0, weight_decay=0, eps=1e-6):\n",
    "        '''\n",
    "        An abstract optimizer class for handling various kinds of optimizers.\n",
    "        You can specify the optimizer type and related parameters as you want.\n",
    "        Usage is exactly the same as an instance of torch.optim\n",
    "\n",
    "        Args:\n",
    "            params: torch.nn.Parameter. The NN parameters to optimize\n",
    "            optimizer_type: type of the optimizer to use\n",
    "            lr: learning rate\n",
    "            momentum: momentum, if needed\n",
    "            weight_decay: weight decay, if needed. Equivalent to L2 regulariztion.\n",
    "            eps: eps parameter, if needed.\n",
    "        '''\n",
    "        if optimizer_type == 'RMSProp':\n",
    "            self.optimizer = optim.RMSprop(params, lr=lr, eps=eps, weight_decay=weight_decay, momentum=momentum)\n",
    "        elif optimizer_type == 'Adagrad':\n",
    "            self.optimizer = optim.Adagrad(params, lr=lr, weight_decay=weight_decay)\n",
    "        elif optimizer_type == 'Adadelta':\n",
    "            self.optimizer = optim.Adadelta(params, lr=lr, eps=eps, weight_decay=weight_decay)\n",
    "        elif optimizer_type == 'Adam':\n",
    "            self.optimizer = optim.Adam(params, lr=lr, eps=eps, weight_decay=weight_decay)\n",
    "        elif optimizer_type == 'SparseAdam':\n",
    "            self.optimizer = optim.SparseAdam(params, lr=lr, eps=eps)\n",
    "        elif optimizer_type == 'SGD':\n",
    "            self.optimizer = optim.SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-minority",
   "metadata": {},
   "source": [
    "### Le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "japanese-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class GRU4REC(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, final_act='tanh',\n",
    "                 dropout_hidden=.5, dropout_input=0, batch_size=50, embedding_dim=-1, use_cuda=False):\n",
    "        super(GRU4REC, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_hidden = dropout_hidden\n",
    "        self.dropout_input = dropout_input\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.use_cuda = use_cuda\n",
    "        self.device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "        self.onehot_buffer = self.init_emb()\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.create_final_activation(final_act)\n",
    "        if self.embedding_dim != -1:\n",
    "            self.look_up = nn.Embedding(input_size, self.embedding_dim)\n",
    "            self.gru = nn.GRU(self.embedding_dim, self.hidden_size, self.num_layers, dropout=self.dropout_hidden)\n",
    "        else:\n",
    "            self.gru = nn.GRU(self.input_size, self.hidden_size, self.num_layers, dropout=self.dropout_hidden)\n",
    "        self = self.to(self.device)\n",
    "\n",
    "    def create_final_activation(self, final_act):\n",
    "        \"\"\"Set the final activation function of the network\n",
    "        \"\"\"\n",
    "        if final_act == 'tanh':\n",
    "            self.final_activation = nn.Tanh()\n",
    "        elif final_act == 'relu':\n",
    "            self.final_activation = nn.ReLU()\n",
    "        elif final_act == 'softmax':\n",
    "            self.final_activation = nn.Softmax()\n",
    "        elif final_act == 'softmax_logit':\n",
    "            self.final_activation = nn.LogSoftmax()\n",
    "        elif final_act.startswith('elu-'):\n",
    "            self.final_activation = nn.ELU(alpha=float(final_act.split('-')[1]))\n",
    "        elif final_act.startswith('leaky-'):\n",
    "            self.final_activation = nn.LeakyReLU(negative_slope=float(final_act.split('-')[1]))\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        '''\n",
    "        Args:\n",
    "            input (B,): a batch of item indices from a session-parallel mini-batch.\n",
    "            target (B,): torch.LongTensor of next item indices from a session-parallel mini-batch.\n",
    "\n",
    "        Returns:\n",
    "            logit (B,C): Variable that stores the logits for the next items in the session-parallel mini-batch\n",
    "            hidden: GRU hidden state\n",
    "        '''\n",
    "\n",
    "        if self.embedding_dim == -1:\n",
    "            embedded = self.onehot_encode(input)\n",
    "            if self.training and self.dropout_input > 0: embedded = self.embedding_dropout(embedded)\n",
    "            embedded = embedded.unsqueeze(0)\n",
    "        else:\n",
    "            embedded = input.unsqueeze(0)\n",
    "            embedded = self.look_up(embedded)\n",
    "\n",
    "        output, hidden = self.gru(embedded, hidden) #(num_layer, B, H)\n",
    "        output = output.view(-1, output.size(-1))  #(B,H)\n",
    "        logit = self.final_activation(self.h2o(output))\n",
    "\n",
    "        return logit, hidden\n",
    "\n",
    "    def init_emb(self):\n",
    "        '''\n",
    "        Initialize the one_hot embedding buffer, which will be used for producing the one-hot embeddings efficiently\n",
    "        '''\n",
    "        onehot_buffer = torch.FloatTensor(self.batch_size, self.output_size)\n",
    "        onehot_buffer = onehot_buffer.to(self.device)\n",
    "        return onehot_buffer\n",
    "\n",
    "    def onehot_encode(self, input):\n",
    "        \"\"\"\n",
    "        Returns a one-hot vector corresponding to the input\n",
    "        Args:\n",
    "            input (B,): torch.LongTensor of item indices\n",
    "            buffer (B,output_size): buffer that stores the one-hot vector\n",
    "        Returns:\n",
    "            one_hot (B,C): torch.FloatTensor of one-hot vectors\n",
    "        \"\"\"\n",
    "        self.onehot_buffer.zero_()\n",
    "        index = input.view(-1, 1)\n",
    "        one_hot = self.onehot_buffer.scatter_(1, index, 1)\n",
    "        return one_hot\n",
    "\n",
    "    def embedding_dropout(self, input):\n",
    "        p_drop = torch.Tensor(input.size(0), 1).fill_(1 - self.dropout_input)\n",
    "        mask = torch.bernoulli(p_drop).expand_as(input) / (1 - self.dropout_input)\n",
    "        mask = mask.to(self.device)\n",
    "        input = input * mask\n",
    "        return input\n",
    "\n",
    "    def init_hidden(self):\n",
    "        '''\n",
    "        Initialize the hidden state of the GRU\n",
    "        '''\n",
    "        try:\n",
    "            h0 = torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(self.device)\n",
    "        except:\n",
    "            self.device = 'cpu'\n",
    "            h0 = torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(self.device)\n",
    "        return h0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-jersey",
   "metadata": {},
   "source": [
    "#### Calcul des métriques de performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "induced-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall(indices, targets): #recall --> wether next item in session is within top K=20 recommended items or not\n",
    "    \"\"\"\n",
    "    Calculates the recall score for the given predictions and targets\n",
    "    Args:\n",
    "        indices (Bxk): torch.LongTensor. top-k indices predicted by the model.\n",
    "        targets (B): torch.LongTensor. actual target indices.\n",
    "    Returns:\n",
    "        recall (float): the recall score\n",
    "    \"\"\"\n",
    "    targets = targets.view(-1, 1).expand_as(indices)\n",
    "    hits = (targets == indices).nonzero()\n",
    "    if len(hits) == 0:\n",
    "        return 0\n",
    "    n_hits = (targets == indices).nonzero()[:, :-1].size(0)\n",
    "    recall = float(n_hits) / targets.size(0)\n",
    "    return recall\n",
    "\n",
    "\n",
    "def get_mrr(indices, targets): #Mean Receiprocal Rank --> Average of rank of next item in the session.\n",
    "    \"\"\"\n",
    "    Calculates the MRR score for the given predictions and targets\n",
    "    Args:\n",
    "        indices (Bxk): torch.LongTensor. top-k indices predicted by the model.\n",
    "        targets (B): torch.LongTensor. actual target indices.\n",
    "    Returns:\n",
    "        mrr (float): the mrr score\n",
    "    \"\"\"\n",
    "    tmp = targets.view(-1, 1)\n",
    "    targets = tmp.expand_as(indices)\n",
    "    hits = (targets == indices).nonzero()\n",
    "    ranks = hits[:, -1] + 1\n",
    "    ranks = ranks.float()\n",
    "    rranks = torch.reciprocal(ranks)\n",
    "    mrr = torch.sum(rranks).data / targets.size(0)\n",
    "    return mrr.item()\n",
    "\n",
    "\n",
    "def evaluate(indices, targets, k=20):\n",
    "    \"\"\"\n",
    "    Evaluates the model using Recall@K, MRR@K scores.\n",
    "\n",
    "    Args:\n",
    "        logits (B,C): torch.LongTensor. The predicted logit for the next items.\n",
    "        targets (B): torch.LongTensor. actual target indices.\n",
    "\n",
    "    Returns:\n",
    "        recall (float): the recall score\n",
    "        mrr (float): the mrr score\n",
    "    \"\"\"\n",
    "    _, indices = torch.topk(indices, k, -1)\n",
    "    recall = get_recall(indices, targets)\n",
    "    mrr = get_mrr(indices, targets)\n",
    "    return recall, mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-halloween",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "working-scene",
   "metadata": {},
   "source": [
    "#### fonction d'entrainement et d'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "unlike-alfred",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation(object):\n",
    "    def __init__(self, model, loss_func, use_cuda, k=20):\n",
    "        self.model = model\n",
    "        self.loss_func = loss_func\n",
    "        self.topk = k\n",
    "        self.device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "    def eval(self, eval_data, batch_size):\n",
    "        self.model.eval()\n",
    "        losses = []\n",
    "        recalls = []\n",
    "        mrrs = []\n",
    "        dataloader = DataLoader(eval_data, batch_size)\n",
    "        with torch.no_grad():\n",
    "            hidden = self.model.init_hidden()\n",
    "            for ii, (inputs, target, mask) in tqdm(enumerate(dataloader), total=len(dataloader.dataset.df) // dataloader.batch_size, miniters = 1000):\n",
    "            #for input, target, mask in dataloader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                target = target.to(self.device)\n",
    "                logit, hidden = self.model(inputs, hidden)\n",
    "                logit_sampled = logit[:, target.view(-1)]\n",
    "                loss = self.loss_func(logit_sampled)\n",
    "                recall, mrr = evaluate(logit, target, k=self.topk)\n",
    "\n",
    "                # torch.Tensor.item() to get a Python number from a tensor containing a single value\n",
    "                losses.append(loss.item())\n",
    "                recalls.append(recall)\n",
    "                mrrs.append(mrr)\n",
    "        mean_losses = np.mean(losses)\n",
    "        mean_recall = np.mean(recalls)\n",
    "        mean_mrr = np.mean(mrrs)\n",
    "\n",
    "        return mean_losses, mean_recall,mean_mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "under-parade",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model, train_data, eval_data, optim, use_cuda, loss_func, batch_size,k_eval):\n",
    "        self.model = model\n",
    "        self.train_data = train_data\n",
    "        self.eval_data = eval_data\n",
    "        self.optim = optim\n",
    "        self.loss_func = loss_func\n",
    "        self.evaluation = Evaluation(self.model, self.loss_func, use_cuda, k = k_eval)\n",
    "        self.device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "        self.batch_size = batch_size\n",
    "        #self.args = args\n",
    "\n",
    "    def train(self, start_epoch, end_epoch, start_time=None):\n",
    "        results=[]\n",
    "        if start_time is None:\n",
    "            self.start_time = time.time()\n",
    "        else:\n",
    "            self.start_time = start_time\n",
    "\n",
    "        for epoch in range(start_epoch, end_epoch + 1):\n",
    "            st = time.time()\n",
    "            print('Start Epoch #', epoch)\n",
    "            train_loss = self.train_epoch(epoch)\n",
    "            loss, recall, mrr = self.evaluation.eval(self.eval_data, self.batch_size)\n",
    "            #loss, recall,mrr= self.evaluation.eval(self.eval_data, self.batch_size)\n",
    "            \n",
    "\n",
    "\n",
    "            print(\"Epoch: {}, train loss: {:.4f}, loss: {:.4f}, recall: {:.4f}, mrr: {:.4f}, time: {}\".format(epoch, train_loss, loss, recall, mrr, time.time() - st))\n",
    "            results.append([train_loss,loss,recall,mrr])\n",
    "        return results\n",
    "            #checkpoint = {\n",
    "                #'model': self.model,\n",
    "                #'args': self.args,\n",
    "                #'epoch': epoch,\n",
    "                #'optim': self.optim,\n",
    "                #'loss': loss,\n",
    "                #'recall': recall,\n",
    "                #'mrr': mrr\n",
    "            #}\n",
    "            #model_name = os.path.join(self.args.checkpoint_dir, \"model_{0:05d}.pt\".format(epoch))\n",
    "            #torch.save(checkpoint, model_name)\n",
    "            #print(\"Save model as %s\" % model_name)\n",
    "\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "\n",
    "        def reset_hidden(hidden, mask):\n",
    "            \"\"\"Helper function that resets hidden state when some sessions terminate\"\"\"\n",
    "            if len(mask) != 0:\n",
    "                hidden[:, mask, :] = 0\n",
    "            return hidden\n",
    "\n",
    "        hidden = self.model.init_hidden()\n",
    "        dataloader = DataLoader(self.train_data, self.batch_size)\n",
    "        #for ii,(data,label) in tqdm(enumerate(train_dataloader),total=len(train_data)):\n",
    "        for ii, (inputs, target, mask) in tqdm(enumerate(dataloader), total=len(dataloader.dataset.df) // dataloader.batch_size, miniters = 1000):\n",
    "            inputs = inputs.to(self.device)\n",
    "            target = target.to(self.device)\n",
    "            self.optim.zero_grad()\n",
    "            hidden = reset_hidden(hidden, mask).detach()\n",
    "            logit, hidden = self.model(inputs, hidden)\n",
    "            # output sampling\n",
    "            logit_sampled = logit[:, target.view(-1)]\n",
    "            loss = self.loss_func(logit_sampled)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "\n",
    "        mean_losses = np.mean(losses)\n",
    "        return mean_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-expression",
   "metadata": {},
   "source": [
    "#### Entrainements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "assisted-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "##reading of training data and evaluation data\n",
    "train_data = Dataset('rsc15_train_tr.txt')\n",
    "valid_data = Dataset('rsc15_train_valid.txt', itemmap=train_data.itemmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "skilled-webcam",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialisation of hyper parameters\n",
    "input_size = len(train_data.items)\n",
    "hidden_size = 100\n",
    "num_layers = 3\n",
    "output_size = input_size\n",
    "batch_size = 32\n",
    "dropout_input = 0\n",
    "dropout_hidden = 0.5\n",
    "embedding_dim = -1\n",
    "final_act = 'tanh'\n",
    "loss_type = 'TOP1-max'\n",
    "optimizer_type = 'Adagrad'\n",
    "lr = 0.01\n",
    "weight_decay = 0\n",
    "momentum = 0\n",
    "eps = 1e-6\n",
    "n_epochs = 2\n",
    "time_sort = False\n",
    "cuda = torch.cuda.is_available()\n",
    "is_eval = False\n",
    "k_eval=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "eight-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialisation of loss function\n",
    "loss_function = LossFunction(loss_type=loss_type, use_cuda=cuda) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-course",
   "metadata": {},
   "source": [
    "First training with TOP1-max loss function on 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "surprised-poster",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                       | 0/986843 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### START TRAINING....\n",
      "Start Epoch # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████                  | 738282/986843 [1:38:55<33:18, 124.38it/s]\n",
      " 79%|████████████████████████████████████████████████████████████▌                | 1430/1819 [00:03<00:00, 443.79it/s]\n",
      "  0%|                                                                                       | 0/986843 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 0.0281, loss: 0.0288, recall: 0.3393, mrr: 1.0000, time: 5939.15017080307\n",
      "Start Epoch # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████                  | 738282/986843 [1:39:32<33:30, 123.61it/s]\n",
      " 79%|████████████████████████████████████████████████████████████▌                | 1430/1819 [00:03<00:00, 441.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: 0.0277, loss: 0.0287, recall: 0.3962, mrr: 1.0000, time: 5976.10270524025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if not is_eval: #training\n",
    "        #Initialize the model\n",
    "        model = GRU4REC(input_size, hidden_size, output_size, final_act=final_act,\n",
    "                            num_layers=num_layers, use_cuda=cuda, batch_size=batch_size,\n",
    "                            dropout_input=dropout_input, dropout_hidden=dropout_hidden, embedding_dim=embedding_dim).to('cuda')\n",
    "        #weights initialization\n",
    "        #init_model(model)\n",
    "        #optimizer\n",
    "        optimizer = Optimizer(model.parameters(), optimizer_type=optimizer_type, lr=lr,\n",
    "                                  weight_decay=weight_decay, momentum=momentum, eps=eps)\n",
    "        #trainer class\n",
    "        trainer = Trainer(model, train_data=train_data, eval_data=valid_data, optim=optimizer,\n",
    "                              use_cuda=cuda, loss_func=loss_function, batch_size=batch_size,k_eval=k_eval)\n",
    "        print('#### START TRAINING....')\n",
    "        result1 = trainer.train(0, n_epochs - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-witness",
   "metadata": {},
   "source": [
    "Nous faisons les première expériences sur toutes les fonctions de perte (BPR, BPR-max, TOP1, TOP1-max, Cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "compressed-nelson",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_type = 'BPR-max'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-northern",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                       | 0/986843 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### START TRAINING....\n",
      "Start Epoch # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████▊                  | 738282/986843 [2:19:21<46:55, 88.29it/s]\n",
      " 79%|████████████████████████████████████████████████████████████▌                | 1430/1819 [00:04<00:01, 311.79it/s]\n",
      "  0%|                                                                                       | 0/986843 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 0.0281, loss: 0.0289, recall: 0.3100, mrr: 0.0813, time: 8366.553636550903\n",
      "Start Epoch # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|███████████████████████████████████                                  | 502167/986843 [1:15:04<1:10:24, 114.72it/s]"
     ]
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "if not is_eval: #training\n",
    "        #Initialize the model\n",
    "        model = GRU4REC(input_size, hidden_size, output_size, final_act=final_act,\n",
    "                            num_layers=num_layers, use_cuda=cuda, batch_size=batch_size,\n",
    "                            dropout_input=dropout_input, dropout_hidden=dropout_hidden, embedding_dim=embedding_dim).to('cuda')\n",
    "        #weights initialization\n",
    "        #init_model(model)\n",
    "        #optimizer\n",
    "        optimizer = Optimizer(model.parameters(), optimizer_type=optimizer_type, lr=lr,\n",
    "                                  weight_decay=weight_decay, momentum=momentum, eps=eps)\n",
    "        #trainer class\n",
    "        trainer = Trainer(model, train_data=train_data, eval_data=valid_data, optim=optimizer,\n",
    "                              use_cuda=cuda, loss_func=loss_function, batch_size=batch_size,k_eval=k_eval)\n",
    "        print('#### START TRAINING....')\n",
    "        result1 = trainer.train(0, n_epochs - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-control",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"result1.txt\", \"w\") as fp:\n",
    "    json.dump(result1, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-military",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_type = 'BPR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-internet",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "if not is_eval: #training\n",
    "        #Initialize the model\n",
    "        model2 = GRU4REC(input_size, hidden_size, output_size, final_act=final_act,\n",
    "                            num_layers=num_layers, use_cuda=cuda, batch_size=batch_size,\n",
    "                            dropout_input=dropout_input, dropout_hidden=dropout_hidden, embedding_dim=embedding_dim).to('cuda')\n",
    "        #weights initialization\n",
    "        #init_model(model)\n",
    "        #optimizer\n",
    "        optimizer = Optimizer(model2.parameters(), optimizer_type=optimizer_type, lr=lr,\n",
    "                                  weight_decay=weight_decay, momentum=momentum, eps=eps)\n",
    "        #trainer class\n",
    "        trainer = Trainer(model2, train_data=train_data, eval_data=valid_data, optim=optimizer,\n",
    "                              use_cuda=cuda, loss_func=loss_function, batch_size=batch_size,k_eval=k_eval)\n",
    "        print('#### START TRAINING....')\n",
    "        result2 = trainer.train(0, n_epochs - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"result2.txt\", \"w\") as fp:\n",
    "    json.dump(result2, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-douglas",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_type = 'TOP1-max'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "if not is_eval: #training\n",
    "        #Initialize the model\n",
    "        model3 = GRU4REC(input_size, hidden_size, output_size, final_act=final_act,\n",
    "                            num_layers=num_layers, use_cuda=cuda, batch_size=batch_size,\n",
    "                            dropout_input=dropout_input, dropout_hidden=dropout_hidden, embedding_dim=embedding_dim).to('cuda')\n",
    "        #weights initialization\n",
    "        #init_model(model)\n",
    "        #optimizer\n",
    "        optimizer = Optimizer(model3.parameters(), optimizer_type=optimizer_type, lr=lr,\n",
    "                                  weight_decay=weight_decay, momentum=momentum, eps=eps)\n",
    "        #trainer class\n",
    "        trainer = Trainer(model3, train_data=train_data, eval_data=valid_data, optim=optimizer,\n",
    "                              use_cuda=cuda, loss_func=loss_function, batch_size=batch_size,k_eval=k_eval)\n",
    "        print('#### START TRAINING....')\n",
    "        result3 = trainer.train(0, n_epochs - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"result3.txt\", \"w\") as fp:\n",
    "    json.dump(result3, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_type = 'TOP1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-white",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "if not is_eval: #training\n",
    "        #Initialize the model\n",
    "        model4 = GRU4REC(input_size, hidden_size, output_size, final_act=final_act,\n",
    "                            num_layers=num_layers, use_cuda=cuda, batch_size=batch_size,\n",
    "                            dropout_input=dropout_input, dropout_hidden=dropout_hidden, embedding_dim=embedding_dim).to('cuda')\n",
    "        #weights initialization\n",
    "        #init_model(model)\n",
    "        #optimizer\n",
    "        optimizer = Optimizer(model4.parameters(), optimizer_type=optimizer_type, lr=lr,\n",
    "                                  weight_decay=weight_decay, momentum=momentum, eps=eps)\n",
    "        #trainer class\n",
    "        trainer = Trainer(model4, train_data=train_data, eval_data=valid_data, optim=optimizer,\n",
    "                              use_cuda=cuda, loss_func=loss_function, batch_size=batch_size,k_eval=k_eval)\n",
    "        print('#### START TRAINING....')\n",
    "        result4 = trainer.train(0, n_epochs - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"result4.txt\", \"w\") as fp:\n",
    "    json.dump(result4, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_type = 'SampledCrossEntropyLoss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "if not is_eval: #training\n",
    "        #Initialize the model\n",
    "        model5 = GRU4REC(input_size, hidden_size, output_size, final_act=final_act,\n",
    "                            num_layers=num_layers, use_cuda=cuda, batch_size=batch_size,\n",
    "                            dropout_input=dropout_input, dropout_hidden=dropout_hidden, embedding_dim=embedding_dim).to('cuda')\n",
    "        #weights initialization\n",
    "        #init_model(model)\n",
    "        #optimizer\n",
    "        optimizer = Optimizer(model5.parameters(), optimizer_type=optimizer_type, lr=lr,\n",
    "                                  weight_decay=weight_decay, momentum=momentum, eps=eps)\n",
    "        #trainer class\n",
    "        trainer = Trainer(model5, train_data=train_data, eval_data=valid_data, optim=optimizer,\n",
    "                              use_cuda=cuda, loss_func=loss_function, batch_size=batch_size,k_eval=k_eval)\n",
    "        print('#### START TRAINING....')\n",
    "        result5 = trainer.train(0, n_epochs - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"result5.txt\", \"w\") as fp:\n",
    "    json.dump(result5, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-piece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-application",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-clinton",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-newark",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "specialized-mineral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0280873653445166, 0.0287911825585407, 0.3392701048951049, 1],\n",
       " [0.027731535416971355, 0.02869272900654094, 0.39615384615384613, 1]]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-dance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, model, loss_func, use_cuda, k=20):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "beautiful-conclusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|████████████████████████████████████████████████████████████▌                | 1430/1819 [00:03<00:00, 454.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.02869272900654094, 0.39615384615384613, 0.11811459891862802)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Evaluation(model, loss_function, cuda,  k_eval).eval(valid_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-cuisine",
   "metadata": {},
   "outputs": [],
   "source": [
    ".detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-retention",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_eval: #training\n",
    "        #Initialize the model\n",
    "        model = GRU4REC(input_size, hidden_size, output_size, final_act=final_act,\n",
    "                            num_layers=num_layers, use_cuda=cuda, batch_size=batch_size,\n",
    "                            dropout_input=dropout_input, dropout_hidden=dropout_hidden, embedding_dim=embedding_dim)\n",
    "        #weights initialization\n",
    "        #init_model(model)\n",
    "        #optimizer\n",
    "        optimizer = Optimizer(model.parameters(), optimizer_type=optimizer_type, lr=lr,\n",
    "                                  weight_decay=weight_decay, momentum=momentum, eps=eps)\n",
    "        #trainer class\n",
    "        trainer = Trainer(model, train_data=train_data, eval_data=valid_data, optim=optimizer,\n",
    "                              use_cuda=cuda, loss_func=loss_function, batch_size=batch_size)\n",
    "        print('#### START TRAINING....')\n",
    "        trainer.train(0, n_epochs - 1)\n",
    "    else: #testi\n",
    "        \n",
    "        ng\n",
    "        if args.load_model is not None:\n",
    "            print(\"Loading pre-trained model from {}\".format(args.load_model))\n",
    "            try:\n",
    "                checkpoint = torch.load(args.load_model)\n",
    "            except:\n",
    "                checkpoint = torch.load(args.load_model, map_location=lambda storage, loc: storage)\n",
    "            model = checkpoint[\"model\"]\n",
    "            model.gru.flatten_parameters()\n",
    "            evaluation = lib.Evaluation(model, loss_function, use_cuda=args.cuda, k = args.k_eval)\n",
    "            loss, recall, mrr = evaluation.eval(valid_data, batch_size)\n",
    "            print(\"Final result: recall = {:.2f}, mrr = {:.2f}\".format(recall, mrr))\n",
    "        else:\n",
    "            print(\"No Pretrained Model was found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-spyware",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-integrity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "variable-given",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mbial\\GRU4Rec\n"
     ]
    }
   ],
   "source": [
    "cd GRU4Rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "flying-aviation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: run.py [-h] [-ps PARAM_STRING] [-pf PARAM_PATH] [-l] [-s MODEL_PATH]\n",
      "              [-t TEST_PATH [TEST_PATH ...]] [-m AT [AT ...]] [-e EVAL_TYPE]\n",
      "              [-ss SS] [--sample_store_on_cpu]\n",
      "              [--test_against_items N_TEST_ITEMS]\n",
      "              PATH\n",
      "\n",
      "Train or load a GRU4Rec model & measure recall and MRR on the specified test\n",
      "set(s).\n",
      "\n",
      "positional arguments:\n",
      "  PATH                  Path to the training data (TAB separated file (.tsv or\n",
      "                        .txt) or pickled pandas.DataFrame object (.pickle))\n",
      "                        (if the --load_model parameter is NOT provided) or to\n",
      "                        the serialized model (if the --load_model parameter is\n",
      "                        provided).\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -ps PARAM_STRING, --parameter_string PARAM_STRING\n",
      "                        Training parameters provided as a single parameter\n",
      "                        string. The format of the string is `param_name1=param\n",
      "                        _value1,param_name2=param_value2...`, e.g.: `loss=bpr-\n",
      "                        max,layers=100,constrained_embedding=True`. Boolean\n",
      "                        training parameters should be either True or False;\n",
      "                        parameters that can take a list should use / as the\n",
      "                        separator (e.g. layers=200/200). Mutually exclusive\n",
      "                        with the -pf (--parameter_file) and the -l\n",
      "                        (--load_model) arguments and one of the three must be\n",
      "                        provided.\n",
      "  -pf PARAM_PATH, --parameter_file PARAM_PATH\n",
      "                        Alternatively, training parameters can be set using a\n",
      "                        config file specified in this argument. The config\n",
      "                        file must contain a single OrderedDict named\n",
      "                        `gru4rec_params`. The parameters must have the\n",
      "                        appropriate type (e.g. layers = [100]). Mutually\n",
      "                        exclusive with the -ps (--parameter_string) and the -l\n",
      "                        (--load_model) arguments and one of the three must be\n",
      "                        provided.\n",
      "  -l, --load_model      Load an already trained model instead of training a\n",
      "                        model. Mutually exclusive with the -ps\n",
      "                        (--parameter_string) and the -pf (--parameter_file)\n",
      "                        arguments and one of the three must be provided.\n",
      "  -s MODEL_PATH, --save_model MODEL_PATH\n",
      "                        Save the trained model to the MODEL_PATH. (Default:\n",
      "                        don't save model)\n",
      "  -t TEST_PATH [TEST_PATH ...], --test TEST_PATH [TEST_PATH ...]\n",
      "                        Path to the test data set(s) located at TEST_PATH.\n",
      "                        Multiple test sets can be provided (separate with\n",
      "                        spaces). (Default: don't evaluate the model)\n",
      "  -m AT [AT ...], --measure AT [AT ...]\n",
      "                        Measure recall & MRR at the defined recommendation\n",
      "                        list length(s). Multiple values can be provided.\n",
      "                        (Default: 20)\n",
      "  -e EVAL_TYPE, --eval_type EVAL_TYPE\n",
      "                        Sets how to handle if multiple items in the ranked\n",
      "                        list have the same prediction score (which is usually\n",
      "                        due to saturation or an error). See the documentation\n",
      "                        of evaluate_gpu() in evaluation.py for further\n",
      "                        details. (Default: standard)\n",
      "  -ss SS, --sample_store_size SS\n",
      "                        GRU4Rec uses a buffer for negative samples during\n",
      "                        training to maximize GPU utilization. This parameter\n",
      "                        sets the buffer length. Lower values require more\n",
      "                        frequent recomputation, higher values use more (GPU)\n",
      "                        memory. Unless you know what you are doing, you\n",
      "                        shouldn't mess with this parameter. (Default:\n",
      "                        10000000)\n",
      "  --sample_store_on_cpu\n",
      "                        If provided, the sample store will be stored in the\n",
      "                        RAM instead of the GPU memory. This is not advised in\n",
      "                        most cases, because it significantly lowers the GPU\n",
      "                        utilization. This option is provided if for some\n",
      "                        reason you want to train the model on the CPU (NOT\n",
      "                        advised).\n",
      "  --test_against_items N_TEST_ITEMS\n",
      "                        It is NOT advised to evaluate recommender algorithms\n",
      "                        by ranking a single positive item against a set of\n",
      "                        sampled negatives. It overestimates recommendation\n",
      "                        performance and also skewes comparisons, as it affects\n",
      "                        algorithms differently (and if a different sequence of\n",
      "                        random samples is used, the results are downright\n",
      "                        uncomparable). If testing takes too much time, it is\n",
      "                        advised to sample test sessions to create a smaller\n",
      "                        test set. However, if the number of items is very high\n",
      "                        (i.e. ABOVE FEW MILLIONS), it might be impossible to\n",
      "                        evaluate the model within a reasonable time, even on a\n",
      "                        smaller (but still representative) test set. In this\n",
      "                        case, and this case only, one can sample items to\n",
      "                        evaluate against. This option allows to rank the\n",
      "                        positive item against the N_TEST_ITEMS most popular\n",
      "                        items. This has a lesser effect on comparison and it\n",
      "                        is a much stronger criteria than ranking against\n",
      "                        randomly sampled items. Keep in mind that the real\n",
      "                        performcance of the algorithm will still be\n",
      "                        overestimated by the results, but comparison will be\n",
      "                        mostly fair. If used, you should NEVER SET THIS\n",
      "                        PARAMETER BELOW 50000 and try to set it as high as\n",
      "                        possible (for your required evaluation time).\n",
      "                        (Default: all items are used as negatives for\n",
      "                        evaluation)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "stty: 'standard input': Inappropriate ioctl for device\n",
      "stty: 'standard input': Inappropriate ioctl for device\n",
      "stty: 'standard input': Inappropriate ioctl for device\n",
      "stty: 'standard input': Inappropriate ioctl for device\n",
      "stty: 'standard input': Inappropriate ioctl for device\n",
      "stty: 'standard input': Inappropriate ioctl for device\n",
      "stty: 'standard input': Inappropriate ioctl for device\n",
      "stty: 'standard input': Inappropriate ioctl for device\n",
      "stty: 'standard input': Inappropriate ioctl for device\n",
      "stty: 'standard input': Inappropriate ioctl for device\n",
      "stty: 'standard input': Inappropriate ioctl for device\n",
      "stty: 'standard input': Inappropriate ioctl for device\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    }
   ],
   "source": [
    "!python GRU4Rec/run.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "controversial-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = 'C:/Users/mbial/OneDrive/Bureau/2020_2021/2020-2021/ENSAE/projet/datasets/RSC15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sufficient-spyware",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SET   loss                    TO   bpr-max   (type: <class 'str'>)\n",
      "SET   final_act               TO   elu-0.5   (type: <class 'str'>)\n",
      "SET   hidden_act              TO   tanh      (type: <class 'str'>)\n",
      "SET   layers                  TO   [100]     (type: <class 'list'>)\n",
      "SET   adapt                   TO   adagrad   (type: <class 'str'>)\n",
      "SET   n_epochs                TO   10        (type: <class 'int'>)\n",
      "SET   batch_size              TO   32        (type: <class 'int'>)\n",
      "SET   dropout_p_embed         TO   0.0       (type: <class 'float'>)\n",
      "SET   dropout_p_hidden        TO   0.0       (type: <class 'float'>)\n",
      "SET   learning_rate           TO   0.2       (type: <class 'float'>)\n",
      "SET   momentum                TO   0.3       (type: <class 'float'>)\n",
      "SET   n_sample                TO   2048      (type: <class 'int'>)\n",
      "SET   sample_alpha            TO   0.0       (type: <class 'float'>)\n",
      "SET   bpreg                   TO   1.0       (type: <class 'float'>)\n",
      "SET   constrained_embedding   TO   False     (type: <class 'bool'>)\n",
      "Loading training data...\n",
      "Loading data from TAB separated file: rsc15_train_tr.txt\n",
      "Started training\n",
      "The dataframe is not sorted by SessionId, sorting now\n",
      "Data is sorted in 25.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "stty: 'standard input': Inappropriate ioctl for device\n",
      "stty: 'standard input': Inappropriate ioctl for device\n",
      "stty: 'standard input': Inappropriate ioctl for device\n",
      "stty: 'standard input': Inappropriate ioctl for device\n",
      "stty: 'standard input': Inappropriate ioctl for device\n",
      "stty: 'standard input': Inappropriate ioctl for device\n",
      "stty: 'standard input': Inappropriate ioctl for device\n",
      "stty: 'standard input': Inappropriate ioctl for device\n",
      "stty: 'standard input': Inappropriate ioctl for device\n",
      "stty: 'standard input': Inappropriate ioctl for device\n",
      "stty: 'standard input': Inappropriate ioctl for device\n",
      "stty: 'standard input': Inappropriate ioctl for device\n",
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "c:\\users\\mbial\\anaconda3\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "ERROR (theano.gpuarray): Could not initialize pygpu, support disabled\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\mbial\\anaconda3\\lib\\site-packages\\theano\\gpuarray\\__init__.py\", line 227, in <module>\n",
      "    use(config.device)\n",
      "  File \"c:\\users\\mbial\\anaconda3\\lib\\site-packages\\theano\\gpuarray\\__init__.py\", line 214, in use\n",
      "    init_dev(device, preallocate=preallocate)\n",
      "  File \"c:\\users\\mbial\\anaconda3\\lib\\site-packages\\theano\\gpuarray\\__init__.py\", line 67, in init_dev\n",
      "    raise RuntimeError(\"The new gpu-backend need a c++ compiler.\")\n",
      "RuntimeError: The new gpu-backend need a c++ compiler.\n",
      "Traceback (most recent call last):\n",
      "  File \"GRU4Rec/run.py\", line 105, in <module>\n",
      "    gru.fit(data, sample_store=args.sample_store_size, store_type='gpu')\n",
      "  File \"C:\\Users\\mbial\\OneDrive\\Bureau\\2020_2021\\2020-2021\\ENSAE\\projet\\datasets\\RSC15\\GRU4Rec\\gru4rec.py\", line 554, in fit\n",
      "    updates_st[self.ST] = gpu_searchsorted(P, X, dtype_int64=True).reshape((generate_length, self.n_sample))\n",
      "  File \"C:\\Users\\mbial\\OneDrive\\Bureau\\2020_2021\\2020-2021\\ENSAE\\projet\\datasets\\RSC15\\GRU4Rec\\gpu_ops.py\", line 43, in gpu_searchsorted\n",
      "    return cto.GpuBinarySearchSorted(dtype_int64=dtype_int64)(P, X)\n",
      "  File \"c:\\users\\mbial\\anaconda3\\lib\\site-packages\\theano\\gof\\op.py\", line 615, in __call__\n",
      "    node = self.make_node(*inputs, **kwargs)\n",
      "  File \"C:\\Users\\mbial\\OneDrive\\Bureau\\2020_2021\\2020-2021\\ENSAE\\projet\\datasets\\RSC15\\GRU4Rec\\custom_theano_ops.py\", line 292, in make_node\n",
      "    d = as_gpuarray_variable(d, context_name=self.context_name)\n",
      "  File \"c:\\users\\mbial\\anaconda3\\lib\\site-packages\\theano\\gpuarray\\basic_ops.py\", line 79, in as_gpuarray_variable\n",
      "    return copy_stack_trace(x, GpuFromHost(context_name)(x))\n",
      "  File \"c:\\users\\mbial\\anaconda3\\lib\\site-packages\\theano\\gof\\op.py\", line 615, in __call__\n",
      "    node = self.make_node(*inputs, **kwargs)\n",
      "  File \"c:\\users\\mbial\\anaconda3\\lib\\site-packages\\theano\\gpuarray\\basic_ops.py\", line 672, in make_node\n",
      "    out_var = GpuArrayType(broadcastable=x.broadcastable,\n",
      "  File \"c:\\users\\mbial\\anaconda3\\lib\\site-packages\\theano\\gpuarray\\type.py\", line 186, in __init__\n",
      "    get_context(self.context_name)\n",
      "  File \"c:\\users\\mbial\\anaconda3\\lib\\site-packages\\theano\\gpuarray\\type.py\", line 104, in get_context\n",
      "    raise ContextNotDefined(\"context name %s not defined\" % (name,))\n",
      "theano.gpuarray.type.ContextNotDefined: context name None not defined\n"
     ]
    }
   ],
   "source": [
    "!python GRU4Rec/run.py rsc15_train_tr.txt -t rsc15_train_valid.txt -m 1 5 10 20 -ps loss=bpr-max,final_act=elu-0.5,hidden_act=tanh,layers=100,adapt=adagrad,n_epochs=10,batch_size=32,dropout_p_embed=0.0,dropout_p_hidden=0.0,learning_rate=0.2,momentum=0.3,n_sample=2048,sample_alpha=0.0,bpreg=1.0,constrained_embedding=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-reform",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-tooth",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
