{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "continent-spiritual",
   "metadata": {},
   "source": [
    "\n",
    "# Recurrent Neural Networks with Top-k Gains for Session-based Recommendations\n",
    "\n",
    "# Sommaire\n",
    "* [INTRODUCTION](#intro)\n",
    "* [présentation de l'article de référence (sus-cité) et le nouvel algortihme dévellopé par les auteurs](#pres_auteur)\n",
    "* [Nouvel algorithme développé par les auteurs](#nouvel_algo)\n",
    "    * [Algorithme d'échantillonnage](#algo_echant)\n",
    "    * [Nouvelles fonctions de perte](#nouvel_perte)\n",
    "* [Principaux résultats des auteurs](#resultats)\n",
    "* [Implémentation de l'article](#imple)  \n",
    "    * [Présentation du jeu de données utilisé](#data)\n",
    "    * [Expérimentations](#experi)\n",
    "\n",
    "    * [importation des librairies utiles](#package)\n",
    "    * [Préprocessing des données](#predata)\n",
    "    * [Expérimentations](#experiment)\n",
    "    * [Prise en main des données](#pem)\n",
    "    * [Fonctions de perte](#fdp)\n",
    "    * [Optimiseurs](#op)\n",
    "    * [Modèle](#mod)\n",
    "    * [Métriques](#metrique)\n",
    "    * [Fonction d'entrainement et Evaluation](#train_eval)\n",
    "* [Conclusion](#conclusion)\n",
    "\n",
    " \n",
    "\n",
    "<a class=\"anchor\" id=\"tada\"></a>\n",
    "\n",
    "## INTRODUCTION <a class=\"anchor\" id=\"intro\"></a>\n",
    "Les systèmes de recommandation sont une forme spécifique de filtrage de l'information visant à présenter les éléments d'information qui sont susceptibles d'intéresser l'utilisateur. Ainsi, les systèmes de recommandation sont des algorithmes dont le but est de faire des suggestions pertinentes à l'usager. L’émergence de l’apprentissage automatique (machine learning) avec les technologies du big data et les hautes performances informatiques et de communication offrent de nouvelles perspectives. Ces perspectives permettent entre autres de comprendre et de faire des recommandations dans des processus très gourmands en données et d'affiner les différentes stratégies des parties prenantes telles que les plateformes de streaming ou encore des ventes en ligne(e-commerce). Les entreprises telles que Netflix et autres, utilisent ce système très efficace basé sur des algorithmes pour faire des suggestions de films/series aux utilisateurs et d'être extrêmement concurrentielles. On distingue deux grandes familles de méthodes pour construire un système de recommandation:\n",
    "- les méthodes par filtrage collaboratif;\n",
    "- les méthodes basées sur le contenu.\n",
    "\n",
    "Les méthodes collaboratives pour les systèmes de recommandation sont des méthodes qui se basent uniquement sur les interactions passées enregistrées entre les utilisateurs et les éléments afin de produire de nouvelles recommandations. Ces interactions sont stockées dans la \"matrice des interactions entre utilisateurs et articles\". L'idée principale qui régit les méthodes collaboratives est que ces interactions passées entre utilisateurs et articles sont suffisantes pour détecter des utilisateurs similaires et/ou des articles similaires et faire des prédictions basées sur ces proximités estimées. Contrairement aux méthodes de collaboration qui reposent uniquement sur les interactions entre l'utilisateur et l'article, les approches basées sur le contenu utilisent des informations supplémentaires sur les utilisateurs et/ou les articles. L'idée derrière ces méthodes est de construire un modèle, basé sur les \"caractéristiques\" disponibles, qui expliquent les interactions observées entre l'utilisateur et les articles. Par exemple, si nous voulons faire des recommandations sur un site de vente en ligne nous pouvons ajouter des variables comme le sexe du visiteur, son age, sa profession... \n",
    "\n",
    "Cependant ces deux familles ont une principale limite: elles sont incapables de faire une recommandation lorsqu'il s'agit d'un nouvel utilisateur. Un utilisateur qui n'a pas d'historique. une solution triviale à ce problème est de faire le item-to-item approche. Avec cette approche on recommande à l'utilisateur des articles qui sont similaires. D'autres solutions ont été dévellopées, la solution d'intérêt dans ce travail est la solution basée sur les réseaux de neuronnes récurents (RNN) dévellopée dans l'article **SESSION-BASED RECOMMENDATIONS WITH RECURRENT NEURAL NETWORKS**.  Les RNN sont réputés pour leur excellente habilité à modéliser des données séquentielles. Avec ces réseaux, l'auteur va modéliser toute la session de l'utilisateur afin de pouvoir faire des prédictions.\n",
    " Le but principal de l'article sur lequel nous travaillons est d'analyser les fonctions de pertes de classement utilisées dans l'article précedemment cité et la méthode d'échantillonnage des échantillons négatifs. Ces analyses conduiront à la construction d'une nouvelle classe de fonction de perte et une nouvelle méthode d'échantillonnage des échantillons négatifs qui augmente la performance des RNN et ceci sans coûts computationnels supplémentaires.\n",
    "\n",
    "Notre travail se divisera en quatre grandes parties:\n",
    "- présentation de l'article de référence (sus-cité) et le nouvel algortihme dévellopé par les auteurs;\n",
    "- présentation des résultats phares des auteurs;\n",
    "- implémentation de l'article\n",
    "- une conclusion et présentation des limites.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## présentation de l'article de référence (sus-cité) et le nouvel algortihme dévellopé par les auteurs <a class=\"anchor\" id=\"pres_auteur\"></a>\n",
    "Plusieurs solutions avaient été proposées pour ce problème notemment celle avec la matrice des articles similaires, et des méthodes avec Reccurent Neural Networks(RNN), les LSTMs et les GRU. Dans cette section nous nous focaliseront sur l'article de référence des auteurs et présenteront leur nouvel agorithme. L'article sur lequel l'auteur a bati son raisonnement est **SESSION-BASED RECOMMENDATIONS WITH RECURRENT NEURAL NETWORKS**, disponible ici [ici](https://arxiv.org/abs/1511.06939). \n",
    "\n",
    "Dans cet article la version de RNN utilisée est celle de General Recurent Unit (GRU); cette version permet de résoudre le problème du gradient qui disparait avec les RNN. Le GRU est une architecture particulière des RNN (forme simplifiée des Long Short Term Memory) permettant de résoudre le problème de vanishing gradient à travers sa mémoire à cours et à long terme. En effet, au fur et à mesure que l’on propage le gradient dans le temps,son ampleur diminue rapidement. C’est-à-dire que plus la séquence d’entrée est longue, plus il devient difficile de saisir l’influence de la première étape. Les gradients vers les premiers points d’entrée disparaissent et sont approximativement égaux à zéro.\n",
    "\n",
    "Le modèle utilisé ici est constitué d'une couche d'embedding, des couches de GRU, des feedforwards layers et la sortie du réseau contient les différents scores des articles prédisant ainsi le prochain article sur lequel l'utilisateur cliquera. L'entrée du réseau est l'état actuel de la session. Ci-dessous une représentation du réseau.\n",
    "<img src=\"architecture.png\"/>\n",
    "\n",
    "Pour l'entrainement du réseau les auteurs considèrent des mini-batches de sessions parrallèles. En effet les RNN sont toujours entrainés sur des batches de données, et la taille des données d'entrée doit être fixe. Ceci ne peut être obtenu avec ce type de données car les sessions n'ont pas les mêmes durées, de plus, vu qu'on veut modéliser la session entière, cela ne fait pas sens de couper une session pour en faire un batch. Pour résoudre ce problème les auteurs utlisent à la fois plusieurs sessions d'utilisateurs et forment des batchs avec avec les éléments des différentes sessions (pour cela ils supposent l'indépendance entre les sessions). Ci-dessous une illustration de la formation des batches.\n",
    "<img src=\"batchs.png\"/>\n",
    "Ensuite pour chaque session il faudra prédire les prochaines sélections de l'utilisateur. Le problème ici est qu'un site peut contenir des milliers d'articles et que ceux qui intéressent vraiment sont ceux qui pourront intéresser l'utilisateur. Si on considèrent tous les articles cela conduirait à un vecteur sparse car les articles jugés (par le réseau) non intéressants pour l'utilisateur auront des probabilités très faibles. Ainsi pour résoudre ce problème avec la sparsité du résultat les auteurs adoptent cette méthode d'échantillonnage des articles. **Ils considèrent tous les autres articles du mini batchs comme des exemples négatifs**. Cette methode d'échantillonnage est la première limite rélévée par les auteurs dans cet algortihme. L'agorithme en soit n'est pas mauvais mais il est assez restrictif pour les raisons suivantes:\n",
    "- Les mini-batchs sont generalement de taille très petite (il s'agit d'un arbitrage entre la vitesse d'exécution de l'algorithme et la qualité des résultats). Ainsi si le nombre d'articles du site est très grand, sélectionner peu d'échantillons négatifs pourrait conduire à avoir que des échantillons non pertinents (la pertinence d'un échantillon sera définies plus bas);\n",
    "- La taille du mini-batch a un effet direct sur l'apprentissage du réseau;\n",
    "- cette méthode n'est pas forcément optimale pour tous les jeux de données.\n",
    "\n",
    "Pour faire la backpropagation avec leur réseau, les auteurs considèrent deux fonctions de perte de classement. Une propriété de ces fonctions de pertes est qu'il y'a apprentissage lorsque le score de l'article d'intérêt n'est pas plus grand que celui des échantillonnages négatifs. Car autrement, il n'y a rien à apprendre et les articles sont déjà bien classés. Les articles avec des scores inférieurs à l'article d'intérêt sont considérés comme non pertinents. La deuxième limites soulévée par les auteurs concerne les fonctions de pertes utilisées. Ci-dessous les fonctions de pertes utilisées par les auteurs de l'articles de reférence.\n",
    "- TOP1: cette perte a été conçue par les auteurs (article de référence) pour cette tâche, elle régularise l'approximation du rang relatif de l'article concerné. Cette perte est donnée par la formule suivante:\n",
    "$$ L_s^{TOP1} = \\frac{1}{N_s} \\sum_{j=1}^{N_s}{\\sigma(\\hat r_{s,j} - \\hat r_{s,i}) + \\sigma(\\hat r_{s,j}^2)} $$\n",
    "La première partie vise à pousser le score cible au-dessus du score des échantillons, tandis que la seconde\n",
    "partie abaisse le score des échantillons négatifs vers zéro. La seconde partie joue le rôle de régularisation, elle va pénaliser les échantillons négatifs avec des hauts scores.\n",
    "\n",
    "- BPR: Bayesian Personalized Ranking. c'est une méthode de factorisation matricielle qui utilise la perte de classement par paire. Il compare le score d'un positif et d'un négatif échantillonné point. Ici on compare le score de l'élément positif avec plusieurs éléments échantillonnés et on utilise leur moyenne comme la perte. Ainsi on compare le score de l'élément positif avec celui des négatifs. la formule de cette perte est donnée ci-dessous:\n",
    "    $$ L_s^{BPR} = \\frac{-1}{N_s} \\sum_{j=1}^{N_s}{log(\\sigma(\\hat r_{s,i} - \\hat r_{s,j}))} $$\n",
    "où $N_s$ est la taille de l'échantillon, $\\hat r_{s,k}$ est le score de l'article k, i est le prochain item (celui qu'on cherche à prédire) et j les échantillons négatifs, $\\sigma$ désigne la fonction sigmoide.\n",
    "\n",
    "Ces deux fonctions de pertes souffrent du problème du gradient qui disparait. En effet considérons les gradients des deux fonctions de pertes:\n",
    "$$\\frac{\\partial L_s^{TOP1}}{\\partial r_i} = \\frac{-1}{N_s} \\sum_{j=1}^{N_s}{\\sigma(\\hat r_{s,j} - \\hat r_{s,i})(1 - \\sigma(\\hat r_{s,j} - \\hat r_{s,i}))}$$\n",
    "\n",
    "$$\\frac{\\partial L_s^{BPR}}{\\partial r_i} = \\frac{-1}{N_s} \\sum_{j=1}^{N_s}{(1 - \\sigma(\\hat r_{s,i} - \\hat r_{s,j}))}$$\n",
    "\n",
    " Si on considère un article non pertinent (ie les articles tels que $r_j<<r_i$ les termes $ \\sigma(\\hat r_{s,j} - \\hat r_{s,i})$ et $1 - \\sigma(\\hat r_{s,i} - \\hat r_{s,j})$ seront très proches de zéro. En effet $r_{s,j}-r_{s,i}$ <0 et donc la valeur de la sigmoide en ce point est très proche de zéro. Si l'échantillonnage contient un nombre très grand d'échantillons négatifs non pertinents le gradient est tout simplement réduit à 0. ceci ne permet pas au réseau d'apprendre malgré la présence des échantillons positifs pertinents. De plus la valeur du gradient est inversemment proportionnelle au nombre d'échantillons négatifs. Un nombre très grand d'échantillons négatifs pousse le gradient vers 0. Cela signifie que même si tous les échantillons négatifs étaient pertinents, leurs mises à jour diminueraient au fur et à mesure que leur nombre augmenterait. \n",
    " \n",
    "Afin d'évaluer la performance de leurs algortihmes, les auteurs utilisent deux métriques: le recall et la MRR.\n",
    "\n",
    "- Le recall  (ou sensibilité) est la proportion des items d'intérêt proposés parmi l'ensemble des items d'intérêts.\n",
    "\n",
    "- Le MRR (Mean Reciprocal Rank): Ici le rang d'une prédiction est le nombre d'échantillons négatifs qui ont une valeur $r_i$ plus grande que la sienne.\n",
    "\n",
    "## Nouvel algorithme développé par les auteurs <a class=\"anchor\" id=\"nouvel_algo\"></a>\n",
    "Le nouvel algortihme est basé sur la correction des limites rélévées précedemment. Avec l'article de référence, les auteurs vont garder la même structure du réseau et les mêmes métriques de performances. Ils apporteront principalement deux contributions:\n",
    "- Une nouvelle méthode d'échantillonnages des échantillons négatifs;\n",
    "- la définition de nouvelles fonctions de pertes.\n",
    "\n",
    "### Algorithme d'échantillonnage: <a class=\"anchor\" id=\"algo_echan\"></a>\n",
    "Les auteurs gardent toujours les autres éléments du batch comme les échantillons négatifs mais aussi échantillonnent un nombre $N_A$ supplémentaire d'échantillons négatifs qu'ils ajoutent aux batchs. Pour choisir ces échantillons supplémentaires, ils se basent sur le support de chaque échantillon $supp_i$ (ici le support d'un item est le nombre de fois qu'il a été sélectionné dans tout le jeu de données). La probabilié de choisir un échantillon négatif est proportionnelle au coeffficent $supp_i^\\alpha$. Ajouter les échantillons négatifs supplémentaires à chaque batchs devrait naturellement augmenter la complexité de l'algorithme, cependant les calculs étant faits en parrallèle ceci n'ajoute rien au temps de calcul.\n",
    "\n",
    "### Nouvelles fonctions de perte: <a class=\"anchor\" id=\"nouvel_perte\"></a>\n",
    "L'idée est d'avoir le score cible avec le score le plus pertinent de l'échantillon, qui est le score maximal de l'échantillon des négatifs. La fonction maximum n'étant pas différentiable on ne pourra pas l'utiliser pour les gradients. Ainsi un softmax score est calculer à la place et donc, chaque échantillon négatifs est pondéré par sa valeur softmax.\n",
    "- TOP1-MAX:\n",
    "$$ L_s^{TOP1-MAX} = \\sum_{j=1}^{N_s}{s_j(\\sigma(\\hat r_{s,j} - \\hat r_{s,i}) + \\sigma(\\hat r_{s,j}^2))} $$\n",
    "\n",
    "- BPR-MAX\n",
    "$$ L_s^{BPR-MAX} = -log(\\sum_{j=1}^{N_s}{s_j\\sigma(\\hat r_{s,i} - \\hat r_{s,j})} $$\n",
    "\n",
    "## Principaux résultats des auteurs <a class=\"anchor\" id=\"resultats\"></a>\n",
    "\n",
    "De manière globale, leur algorithme augmente le MRR et le recall de 35% par rapport aux résultats obtenus dans leur article de référence et de 53% par rapport aux solutions classiques de filtrage collaboratif.\n",
    "\n",
    "\n",
    "Les auteurs évaluent les améliorations des fonctions pertes sur quatres ensembles de données: RSC15 est basé sur l'ensemble de données de RecSys Challange 2015, qui contient des événements de clic et d'achat d'une boutique en ligne. Seules les données relatives aux clics ont été concernées. VIDEO et VIDXL sont des ensembles de données contenant des événements de visionnage provenant d'un service de vidéo en ligne. Enfin, CLASS est un jeu de données contenant les événements de consultation de pages d'articles d'un site de petites annonces en ligne.\n",
    "\n",
    "Les auteurs considèrent comme benchmarks l'algorithme original GRU4Rec, sur lequel ils cherchent à s'améliorer. Ils considérent les résultats avec la perte TOP1 proposée à l'origine et la fonction d'activation tanh sur la sortie comme la ligne de base et indiquent également les performances de item-kNN, une référence naturelle pour la prédiction de l'élément suivant. Les résultats pour RSC15, VIDXL et CLASS sont directement tirés des articles et mesurés avec les hyperparamètres optimaux dans l'article de référence pour VIDEO.\n",
    "\n",
    "Les prémiers résultats portent sur l'effet d'échantillons négatifs supplémentaires sur la précision de la recommandation. Ils obtiennent que la perte TOP1 ne s'accommode pas bien à un grand nombre d'échantillons supplémentaires car la probabilité d'avoir des échantillons pertinents augmente. Mais les performances se dégradent rapidement lorsque la taille de l'échantillon augmente, ce qui entraîne l'inclusion de nombreux échantillons non pertinents. D'autre part, les trois autres pertes réagissent bien à l'ajout d'échantillons supplémentaires. Le point de rendement décroissant se situe autour de quelques milliers d'échantillons supplémentaires pour l'entropie croisée. TOP1-max commence à perdre légèrement en précision après cela. BPR-max s'améliore avec plus d'échantillons jusqu'au bout, mais perd légèrement en précision lorsque tous les éléments sont utilisés. Il est important de noter que le temps computationnel obtenu par les auteurs est faible. Les résultats sont représentés sur le graphique ci-dessous.\n",
    "\n",
    "<img src=\"sample_res.png\"/>\n",
    "\n",
    "La grande amélioration de la précision par rapport aux benchmarks provient de la combinaison d'échantillons supplémentaires et des fonctions de perte (entropie croisée fixe, TOP1-max et BPR-max). La perte d'entropie croisée fixe c'est-à dire sans échantillonnage supplémentaire est toujours légèrement meilleure que TOP1. De plus, l'augmentation avec l'échantillonnage et la fonction de perte appropriée est stupéfiante puisque les meilleurs résultats dépassent la précision du GRU4Rec original de 18 à 37.5% et celle de item-kNN jusqu'à 55%.\n",
    "\n",
    "## Implémentation de l'article <a class=\"anchor\" id=\"imple\"></a>\n",
    "### Présentation du jeu de données utilisé <a class=\"anchor\" id=\"data\"></a>\n",
    "\n",
    "    Pour cette partie nous avons fait quelques expériementations avec l'algorithme proposé par les auteurs. Pour ce faire, nous utilisons le jeu de données RSC15 issu du challenge RecSys Challenge 2015 de kaggle. Le dataset contient une collection de sessions d'un détaillant, où chaque session encapsule les événements de clics effectués par l'utilisateur au cours de la session. Nous avons utilisé le jeu de données contenant juste les clics pendant les différentes sessions. Le jeu de données d'entrainement contient 31,579,006 lignes et 3 colonnes. Les lignes représentent les différentes sessions ; la première colonne contient l'ID de la session, la seconde colonne contient la date et l'heure à laquelle l'utilisateur a cliqué sur un item et la troisième colonne contient les ID des items.\n",
    "\n",
    "Le jeu de données étant très grand, les temps d'entrainement étaient très longs. Pour nos expériences nous avons décidé de challenger les performances des différentes fonctions de pertes introduites par les auteurs par rapport aux anciennes fonctions de pertes. Aussi avons-nous directement considéré les hyper-paramètres optimaux (d'après la littérature)  car les temps d'entrainement ne nous permettait pas des choix au hasard. Après nous avons vérifié l'importance de la taille d'un batch et des fonctions d'activations terminale sur les résultats.\n",
    "\n",
    "### Expérimentations <a class=\"anchor\" id=\"experi\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-polyester",
   "metadata": {},
   "source": [
    "### importation des librairies utiles <a class=\"anchor\" id=\"package\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "molecular-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import lib\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-unemployment",
   "metadata": {},
   "source": [
    "## Nétoyage des données <a class=\"anchor\" id=\"predata\"></a>\n",
    "Dans cette section il est question de nétoyer les données et de les séparer en train, validation, test. Pour cela Nous lisons le fichier contenant toutes les données. Ce fichier contient trois colonnes:\n",
    "- l'identifiant de la session;\n",
    "- le temps;\n",
    "- les identifiants des items sur lesquels l'utilisateur a cliqué pendant la session.\n",
    "\n",
    "De prime abord nous retenons les sessions de longueur au moins égale à deux ensuite les items qui ont été sélectionnés au moins 5 fois puis une selection des sessions au moins égales à 2. Pour séparer le dataset en train et en test il faut éviter de couper une session (cf article) et donc la stratégie utilisée est de mettre dans le fichier train les sessions qui ont été effectuées jusqu'à un moment donné (tmax-86400) et le reste dans le dataset test. La même stratégie est appliquée sur le dataset train pour le séparer en train et en validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "previous-revelation",
   "metadata": {},
   "outputs": [],
   "source": [
    "##reading of the entire dataset\n",
    "data = pd.read_csv('yoochoose-clicks.dat', sep=',', header=None, usecols=[0,1,2], dtype={0:np.int32, 1:str, 2:np.int64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "minimal-attitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['SessionId', 'TimeStr', 'ItemId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beneficial-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Time'] = data.TimeStr.apply(lambda x: dt.datetime.strptime(x, '%Y-%m-%dT%H:%M:%S.%fZ').timestamp()) #This is not UTC. It does not really matter.\n",
    "del(data['TimeStr']) ##delete the column TimeStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "controversial-duration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33003944, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape ##Check the shape of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sorted-match",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionId</th>\n",
       "      <th>ItemId</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>214536502</td>\n",
       "      <td>1.396861e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>214536500</td>\n",
       "      <td>1.396861e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>214536506</td>\n",
       "      <td>1.396861e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>214577561</td>\n",
       "      <td>1.396861e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>214662742</td>\n",
       "      <td>1.396872e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SessionId     ItemId          Time\n",
       "0          1  214536502  1.396861e+09\n",
       "1          1  214536500  1.396861e+09\n",
       "2          1  214536506  1.396861e+09\n",
       "3          1  214577561  1.396861e+09\n",
       "4          2  214662742  1.396872e+09"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "divided-salmon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SessionId\n",
      "1           4\n",
      "2           6\n",
      "3           3\n",
      "4           2\n",
      "6           2\n",
      "           ..\n",
      "11562156    2\n",
      "11562157    2\n",
      "11562158    3\n",
      "11562159    1\n",
      "11562161    1\n",
      "Length: 9249729, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "session_lengths = data.groupby('SessionId').size() #size of each session\n",
    "print(session_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "brief-bulgarian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31744233, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Only keep session with length >1\n",
    "data = data[np.in1d(data.SessionId, session_lengths[session_lengths>1].index)]\n",
    "data.shape #check the new shape of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "signal-authentication",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ItemId\n",
       "214507224     32\n",
       "214507226     13\n",
       "214507228      1\n",
       "214507239      6\n",
       "214507256      1\n",
       "              ..\n",
       "1178835219     1\n",
       "1178835247     1\n",
       "1178835585     1\n",
       "1178835641     1\n",
       "1178837797    12\n",
       "Length: 52069, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_supports = data.groupby('ItemId').size() #number of time each item was selected\n",
    "item_supports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "mechanical-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Only keep items selected at least 5 times\n",
    "data = data[np.in1d(data.ItemId, item_supports[item_supports>=5].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "critical-thought",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31713448, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape #check new data shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "described-silence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check session length\n",
    "session_lengths = data.groupby('SessionId').size()\n",
    "np.min(session_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "impressed-potter",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Only keep session with length >1\n",
    "data = data[np.in1d(data.SessionId, session_lengths[session_lengths>=2].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "variable-catholic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1412038799.43"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmax = data.Time.max() #check the max time of sessions\n",
    "tmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fourth-jonathan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SessionId\n",
       "1           1.396861e+09\n",
       "2           1.396872e+09\n",
       "3           1.396438e+09\n",
       "4           1.396866e+09\n",
       "6           1.396797e+09\n",
       "                ...     \n",
       "11562152    1.411718e+09\n",
       "11562153    1.411571e+09\n",
       "11562156    1.411700e+09\n",
       "11562157    1.411641e+09\n",
       "11562158    1.411701e+09\n",
       "Name: Time, Length: 7981581, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_max_times = data.groupby('SessionId').Time.max()\n",
    "session_max_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "strategic-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_train = session_max_times[session_max_times < tmax-86400].index ##selected index for train data\n",
    "session_test = session_max_times[session_max_times >= tmax-86400].index ##selected index for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "integrated-assurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[np.in1d(data.SessionId, session_train)] ##train data\n",
    "test = data[np.in1d(data.SessionId, session_test)]\n",
    "test = test[np.in1d(test.ItemId, train.ItemId)] ##test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "regional-sperm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionId</th>\n",
       "      <th>ItemId</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32186847</th>\n",
       "      <td>11265009</td>\n",
       "      <td>214586805</td>\n",
       "      <td>1.411997e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32186848</th>\n",
       "      <td>11265009</td>\n",
       "      <td>214509260</td>\n",
       "      <td>1.411997e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32186868</th>\n",
       "      <td>11265017</td>\n",
       "      <td>214857547</td>\n",
       "      <td>1.412011e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32186869</th>\n",
       "      <td>11265017</td>\n",
       "      <td>214857268</td>\n",
       "      <td>1.412011e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32186870</th>\n",
       "      <td>11265017</td>\n",
       "      <td>214857260</td>\n",
       "      <td>1.412011e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003915</th>\n",
       "      <td>11299816</td>\n",
       "      <td>214859859</td>\n",
       "      <td>1.412014e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003916</th>\n",
       "      <td>11299816</td>\n",
       "      <td>214859859</td>\n",
       "      <td>1.412014e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003917</th>\n",
       "      <td>11299816</td>\n",
       "      <td>214859859</td>\n",
       "      <td>1.412014e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003918</th>\n",
       "      <td>11299816</td>\n",
       "      <td>214746399</td>\n",
       "      <td>1.412015e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003919</th>\n",
       "      <td>11299816</td>\n",
       "      <td>214567327</td>\n",
       "      <td>1.412016e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71222 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          SessionId     ItemId          Time\n",
       "32186847   11265009  214586805  1.411997e+09\n",
       "32186848   11265009  214509260  1.411997e+09\n",
       "32186868   11265017  214857547  1.412011e+09\n",
       "32186869   11265017  214857268  1.412011e+09\n",
       "32186870   11265017  214857260  1.412011e+09\n",
       "...             ...        ...           ...\n",
       "33003915   11299816  214859859  1.412014e+09\n",
       "33003916   11299816  214859859  1.412014e+09\n",
       "33003917   11299816  214859859  1.412014e+09\n",
       "33003918   11299816  214746399  1.412015e+09\n",
       "33003919   11299816  214567327  1.412016e+09\n",
       "\n",
       "[71222 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fuzzy-horizon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionId</th>\n",
       "      <th>ItemId</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>214536502</td>\n",
       "      <td>1.396861e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>214536500</td>\n",
       "      <td>1.396861e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>214536506</td>\n",
       "      <td>1.396861e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>214577561</td>\n",
       "      <td>1.396861e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>214662742</td>\n",
       "      <td>1.396872e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003939</th>\n",
       "      <td>11299809</td>\n",
       "      <td>214819412</td>\n",
       "      <td>1.411630e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003940</th>\n",
       "      <td>11299809</td>\n",
       "      <td>214830939</td>\n",
       "      <td>1.411631e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003941</th>\n",
       "      <td>11299811</td>\n",
       "      <td>214854855</td>\n",
       "      <td>1.411578e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003942</th>\n",
       "      <td>11299811</td>\n",
       "      <td>214854838</td>\n",
       "      <td>1.411578e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003943</th>\n",
       "      <td>11299811</td>\n",
       "      <td>214848658</td>\n",
       "      <td>1.411578e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31637239 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          SessionId     ItemId          Time\n",
       "0                 1  214536502  1.396861e+09\n",
       "1                 1  214536500  1.396861e+09\n",
       "2                 1  214536506  1.396861e+09\n",
       "3                 1  214577561  1.396861e+09\n",
       "4                 2  214662742  1.396872e+09\n",
       "...             ...        ...           ...\n",
       "33003939   11299809  214819412  1.411630e+09\n",
       "33003940   11299809  214830939  1.411631e+09\n",
       "33003941   11299811  214854855  1.411578e+09\n",
       "33003942   11299811  214854838  1.411578e+09\n",
       "33003943   11299811  214848658  1.411578e+09\n",
       "\n",
       "[31637239 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "organic-solution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "tslength = test.groupby('SessionId').size()\n",
    "print(np.min(tslength))\n",
    "test = test[np.in1d(test.SessionId, tslength[tslength>=2].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "prospective-project",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full train set\n",
      "\tEvents: 31637239\n",
      "\tSessions: 7966257\n",
      "\tItems: 37483\n",
      "Test set\n",
      "\tEvents: 71222\n",
      "\tSessions: 15324\n",
      "\tItems: 6751\n"
     ]
    }
   ],
   "source": [
    "##save train data and test data\n",
    "print('Full train set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}'.format(len(train), train.SessionId.nunique(), train.ItemId.nunique()))\n",
    "train.to_csv(PATH_TO_PROCESSED_DATA + 'rsc15_train_full.txt', sep='\\t', index=False)\n",
    "print('Test set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}'.format(len(test), test.SessionId.nunique(), test.ItemId.nunique()))\n",
    "test.to_csv(PATH_TO_PROCESSED_DATA + 'rsc15_test.txt', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "considered-glance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "\tEvents: 31579006\n",
      "\tSessions: 7953885\n",
      "\tItems: 37483\n",
      "Validation set\n",
      "\tEvents: 58233\n",
      "\tSessions: 12372\n",
      "\tItems: 6359\n"
     ]
    }
   ],
   "source": [
    "##same strategy for the selection of validation data\n",
    "##split the previous train data on train data and validation data\n",
    "##save dataframe\n",
    "tmax = train.Time.max()\n",
    "session_max_times = train.groupby('SessionId').Time.max()\n",
    "session_train = session_max_times[session_max_times < tmax-86400].index\n",
    "session_valid = session_max_times[session_max_times >= tmax-86400].index\n",
    "train_tr = train[np.in1d(train.SessionId, session_train)]\n",
    "valid = train[np.in1d(train.SessionId, session_valid)]\n",
    "valid = valid[np.in1d(valid.ItemId, train_tr.ItemId)]\n",
    "tslength = valid.groupby('SessionId').size()\n",
    "valid = valid[np.in1d(valid.SessionId, tslength[tslength>=2].index)]\n",
    "print('Train set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}'.format(len(train_tr), train_tr.SessionId.nunique(), train_tr.ItemId.nunique()))\n",
    "train_tr.to_csv(PATH_TO_PROCESSED_DATA + 'rsc15_train_tr.txt', sep='\\t', index=False)\n",
    "print('Validation set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}'.format(len(valid), valid.SessionId.nunique(), valid.ItemId.nunique()))\n",
    "valid.to_csv(PATH_TO_PROCESSED_DATA + 'rsc15_train_valid.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-brand",
   "metadata": {},
   "source": [
    "### Expérimentations <a class=\"anchor\" id=\"experiment\"></a>\n",
    "\n",
    "#### prise en main des données <a class=\"anchor\" id=\"pem\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "subsequent-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    \"\"\"\n",
    "    Read data,\n",
    "    create indices for items ID\n",
    "    created sorted or no indices for SessionID\n",
    "    \n",
    "    arguments:\n",
    "    ----------\n",
    "    path: The link to find data,\n",
    "    session_key: The name of session ID on dataset\n",
    "    item_key: The name of Item ID on dataset\n",
    "    time_key: The name of Time on dataset\n",
    "    n_sample : Number of sample should be used on dataset if positive\n",
    "    itemmap : list of item indices (none per default)\n",
    "    time_sort: wheter the session ID index should be sorted by time or not\n",
    "    \"\"\"\n",
    "    def __init__(self, path, sep='\\t', session_key='SessionId', item_key='ItemId', time_key='Time', n_sample=-1, itemmap=None, itemstamp=None, time_sort=False):\n",
    "        \n",
    "        self.df = pd.read_csv(path, sep=sep, dtype={session_key: int, item_key: int, time_key: float}) #rezd csv\n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.time_key = time_key\n",
    "        self.time_sort = time_sort\n",
    "        if n_sample > 0:\n",
    "            self.df = self.df[:n_sample]\n",
    "\n",
    "        \n",
    "        self.add_item_indices(itemmap=itemmap) # Add colummn item index to data\n",
    "        \"\"\"\n",
    "        Sort the df by time, and then by session ID. That is, df is sorted by session ID and\n",
    "        clicks within a session are next to each other, where the clicks within a session are time-ordered.\n",
    "        \"\"\"\n",
    "        self.df.sort_values([session_key, time_key], inplace=True)\n",
    "        self.click_offsets = self.get_click_offset()\n",
    "        self.session_idx_arr = self.order_session_idx()\n",
    "\n",
    "    def add_item_indices(self, itemmap=None):\n",
    "        \"\"\"\n",
    "        Add item index column named \"item_idx\" to the df\n",
    "        Args:\n",
    "            itemmap (pd.DataFrame): mapping between the item Ids and indices\n",
    "        \"\"\"\n",
    "        if itemmap is None:\n",
    "            item_ids = self.df[self.item_key].unique()  # numpy ND_array with all item_key\n",
    "            item2idx = pd.Series(data=np.arange(len(item_ids)),\n",
    "                                 index=item_ids) # Numpy array with index of each item_key\n",
    "            # Build itemmap is a DataFrame that have 2 columns (self.item_key, 'item_idx)\n",
    "            itemmap = pd.DataFrame({self.item_key: item_ids,\n",
    "                                   'item_idx': item2idx[item_ids].values})\n",
    "        self.itemmap = itemmap\n",
    "        self.df = pd.merge(self.df, self.itemmap, on=self.item_key, how='inner')\n",
    "\n",
    "    def get_click_offset(self):\n",
    "        \"\"\"return a cumulative sum of sessions' size\n",
    "        \n",
    "        \"\"\"\n",
    "        offsets = np.zeros(self.df[self.session_key].nunique() + 1, dtype=np.int32)\n",
    "        offsets[1:] = self.df.groupby(self.session_key).size().cumsum()\n",
    "        return offsets\n",
    "\n",
    "    def order_session_idx(self):\n",
    "        \"\"\"Return sorted indices by time of session key if mentionned (self.time_sort=True)\n",
    "        else return indices of session key\n",
    "        \"\"\"\n",
    "        if self.time_sort:\n",
    "            sessions_start_time = self.df.groupby(self.session_key)[self.time_key].min().values ##minimum time of each session\n",
    "            session_idx_arr = np.argsort(sessions_start_time) #return indice of session_key that would sort sessions_start_time\n",
    "        else:\n",
    "            session_idx_arr = np.arange(self.df[self.session_key].nunique())\n",
    "        return session_idx_arr\n",
    "\n",
    "    @property\n",
    "    def items(self):\n",
    "        \"\"\"number of unique items on session\n",
    "        \"\"\"\n",
    "        return self.itemmap[self.item_key].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "front-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, dataset, batch_size=50):\n",
    "        \"\"\"\n",
    "        A class for creating session-parallel mini-batches.\n",
    "\n",
    "        Args:\n",
    "             dataset (SessionDataset): the session dataset to generate the batches from\n",
    "             batch_size (int): size of the batch\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\" Returns the iterator for producing session-parallel training mini-batches.\n",
    "\n",
    "        Yields:\n",
    "            input (B,): torch.FloatTensor. Item indices that will be encoded as one-hot vectors later.\n",
    "            target (B,): a Variable that stores the target item indices\n",
    "            masks: Numpy array indicating the positions of the sessions to be terminated\n",
    "        \"\"\"\n",
    "        # initializations\n",
    "        df = self.dataset.df ##dataframe whith sessions, items and times\n",
    "        click_offsets = self.dataset.click_offsets #cumulative number of sessions clicks\n",
    "        session_idx_arr = self.dataset.session_idx_arr ##indices array of each session\n",
    "\n",
    "        iters = np.arange(self.batch_size) #iterations\n",
    "        maxiter = iters.max() ##maximum number of iterations\n",
    "        start = click_offsets[session_idx_arr[iters]] #The begin of the session in same batch on the dataset\n",
    "        end = click_offsets[session_idx_arr[iters] + 1] #The begin of the session out of the batch\n",
    "        mask = []  # indicator for the sessions to be terminated\n",
    "        finished = False\n",
    "\n",
    "        while not finished:\n",
    "            minlen = (end - start).min()\n",
    "            # Item indices(for embedding) for clicks where the first sessions start\n",
    "            idx_target = df.item_idx.values[start]\n",
    "\n",
    "            for i in range(minlen - 1):\n",
    "                # Build inputs & targets\n",
    "                idx_input = idx_target\n",
    "                idx_target = df.item_idx.values[start + i + 1]\n",
    "                inputs = torch.LongTensor(idx_input)\n",
    "                target = torch.LongTensor(idx_target)\n",
    "                yield inputs, target, mask\n",
    "\n",
    "            # click indices where a particular session meets second-to-last element\n",
    "            start = start + (minlen - 1)\n",
    "            # see if how many sessions should terminate\n",
    "            mask = np.arange(len(iters))[(end - start) <= 1]\n",
    "            for idx in mask:\n",
    "                maxiter += 1\n",
    "                if maxiter >= len(click_offsets) - 1:\n",
    "                    finished = True\n",
    "                    break\n",
    "                # update the next starting/ending point\n",
    "                iters[idx] = maxiter\n",
    "                start[idx] = click_offsets[session_idx_arr[maxiter]]\n",
    "                end[idx] = click_offsets[session_idx_arr[maxiter] + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-burning",
   "metadata": {},
   "source": [
    "#### Fonctions de pertes <a class=\"anchor\" id=\"fdp\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "liked-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, loss_type='TOP1', use_cuda=False):\n",
    "        \"\"\" An abstract loss function that can supports custom loss functions compatible with PyTorch.\"\"\"\n",
    "        super(LossFunction, self).__init__()\n",
    "        self.loss_type = loss_type\n",
    "        self.use_cuda = use_cuda\n",
    "        if loss_type == 'CrossEntropy':\n",
    "            self._loss_fn = SampledCrossEntropyLoss(use_cuda)\n",
    "        elif loss_type == 'TOP1':\n",
    "            self._loss_fn = TOP1Loss()\n",
    "        elif loss_type == 'BPR':\n",
    "            self._loss_fn = BPRLoss()\n",
    "        elif loss_type == 'TOP1-max':\n",
    "            self._loss_fn = TOP1_max()\n",
    "        elif loss_type == 'BPR-max':\n",
    "            self._loss_fn = BPR_max()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, logit):\n",
    "        return self._loss_fn(logit)\n",
    "\n",
    "\n",
    "class SampledCrossEntropyLoss(nn.Module):\n",
    "    \"\"\" CrossEntropyLoss with n_classes = batch_size = the number of samples in the session-parallel mini-batch \"\"\"\n",
    "    def __init__(self, use_cuda):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "             use_cuda (bool): whether to use cuda or not\n",
    "        \"\"\"\n",
    "        super(SampledCrossEntropyLoss, self).__init__()\n",
    "        self.xe_loss = nn.CrossEntropyLoss()\n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "    def forward(self, logit):\n",
    "        batch_size = logit.size(1)\n",
    "        target = Variable(torch.arange(batch_size).long())\n",
    "        if self.use_cuda:\n",
    "            target = target.cuda()\n",
    "\n",
    "        return self.xe_loss(logit, target)\n",
    "\n",
    "\n",
    "class BPR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BPRLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logit):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logit (BxB): Variable that stores the logits for the items in the mini-batch\n",
    "                         The first dimension corresponds to the batches, and the second\n",
    "                         dimension corresponds to sampled number of items to evaluate\n",
    "        \"\"\"\n",
    "        # differences between the item scores\n",
    "        diff = logit.diag().view(-1, 1).expand_as(logit) - logit\n",
    "        # final loss\n",
    "        loss = -torch.mean(F.logsigmoid(diff))\n",
    "        return loss\n",
    "\n",
    "\n",
    "class BPR_max(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BPR_max, self).__init__()\n",
    "    def forward(self, logit):\n",
    "        logit_softmax = F.softmax(logit, dim=1)\n",
    "        diff = logit.diag().view(-1, 1).expand_as(logit) - logit\n",
    "        loss = -torch.log(torch.mean(logit_softmax * torch.sigmoid(diff)))\n",
    "        return loss\n",
    "\n",
    "\n",
    "class TOP1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TOP1Loss, self).__init__()\n",
    "    def forward(self, logit):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logit (BxB): Variable that stores the logits for the items in the mini-batch\n",
    "                         The first dimension corresponds to the batches, and the second\n",
    "                         dimension corresponds to sampled number of items to evaluate\n",
    "        \"\"\"\n",
    "        diff = -(logit.diag().view(-1, 1).expand_as(logit) - logit)\n",
    "        loss = torch.sigmoid(diff).mean() + torch.sigmoid(logit ** 2).mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class TOP1_max(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TOP1_max, self).__init__()\n",
    "\n",
    "    def forward(self, logit):\n",
    "        logit_softmax = F.softmax(logit, dim=1)\n",
    "        diff = -(logit.diag().view(-1, 1).expand_as(logit) - logit)\n",
    "        loss = torch.mean(logit_softmax * (torch.sigmoid(diff) + torch.sigmoid(logit ** 2)))\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-lancaster",
   "metadata": {},
   "source": [
    "#### L'optimiseur <a class=\"anchor\" id=\"op\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "spatial-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Optimizer:\n",
    "    def __init__(self, params, optimizer_type='Adagrad', lr=.05,\n",
    "                 momentum=0, weight_decay=0, eps=1e-6):\n",
    "        '''\n",
    "        An abstract optimizer class for handling various kinds of optimizers.\n",
    "        You can specify the optimizer type and related parameters as you want.\n",
    "        Usage is exactly the same as an instance of torch.optim\n",
    "\n",
    "        Args:\n",
    "            params: torch.nn.Parameter. The NN parameters to optimize\n",
    "            optimizer_type: type of the optimizer to use\n",
    "            lr: learning rate\n",
    "            momentum: momentum, if needed\n",
    "            weight_decay: weight decay, if needed. Equivalent to L2 regulariztion.\n",
    "            eps: eps parameter, if needed.\n",
    "        '''\n",
    "        if optimizer_type == 'RMSProp':\n",
    "            self.optimizer = optim.RMSprop(params, lr=lr, eps=eps, weight_decay=weight_decay, momentum=momentum)\n",
    "        elif optimizer_type == 'Adagrad':\n",
    "            self.optimizer = optim.Adagrad(params, lr=lr, weight_decay=weight_decay)\n",
    "        elif optimizer_type == 'Adadelta':\n",
    "            self.optimizer = optim.Adadelta(params, lr=lr, eps=eps, weight_decay=weight_decay)\n",
    "        elif optimizer_type == 'Adam':\n",
    "            self.optimizer = optim.Adam(params, lr=lr, eps=eps, weight_decay=weight_decay)\n",
    "        elif optimizer_type == 'SparseAdam':\n",
    "            self.optimizer = optim.SparseAdam(params, lr=lr, eps=eps)\n",
    "        elif optimizer_type == 'SGD':\n",
    "            self.optimizer = optim.SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-companion",
   "metadata": {},
   "source": [
    "### Le modèle <a class=\"anchor\" id=\"mod\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "liable-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class GRU4REC(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, final_act='tanh',\n",
    "                 dropout_hidden=.5, dropout_input=0, batch_size=50, embedding_dim=-1, use_cuda=False):\n",
    "        super(GRU4REC, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_hidden = dropout_hidden\n",
    "        self.dropout_input = dropout_input\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.use_cuda = use_cuda\n",
    "        self.device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "        self.onehot_buffer = self.init_emb()\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.create_final_activation(final_act)\n",
    "        if self.embedding_dim != -1:\n",
    "            self.look_up = nn.Embedding(input_size, self.embedding_dim)\n",
    "            self.gru = nn.GRU(self.embedding_dim, self.hidden_size, self.num_layers, dropout=self.dropout_hidden)\n",
    "        else:\n",
    "            self.gru = nn.GRU(self.input_size, self.hidden_size, self.num_layers, dropout=self.dropout_hidden)\n",
    "        self = self.to(self.device)\n",
    "\n",
    "    def create_final_activation(self, final_act):\n",
    "        \"\"\"Set the final activation function of the network\n",
    "        \"\"\"\n",
    "        if final_act == 'tanh':\n",
    "            self.final_activation = nn.Tanh()\n",
    "        elif final_act == 'relu':\n",
    "            self.final_activation = nn.ReLU()\n",
    "        elif final_act == 'softmax':\n",
    "            self.final_activation = nn.Softmax()\n",
    "        elif final_act == 'softmax_logit':\n",
    "            self.final_activation = nn.LogSoftmax()\n",
    "        elif final_act.startswith('elu-'):\n",
    "            self.final_activation = nn.ELU(alpha=float(final_act.split('-')[1]))\n",
    "        elif final_act.startswith('leaky-'):\n",
    "            self.final_activation = nn.LeakyReLU(negative_slope=float(final_act.split('-')[1]))\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        '''\n",
    "        Args:\n",
    "            input (B,): a batch of item indices from a session-parallel mini-batch.\n",
    "            target (B,): torch.LongTensor of next item indices from a session-parallel mini-batch.\n",
    "\n",
    "        Returns:\n",
    "            logit (B,C): Variable that stores the logits for the next items in the session-parallel mini-batch\n",
    "            hidden: GRU hidden state\n",
    "        '''\n",
    "\n",
    "        if self.embedding_dim == -1:\n",
    "            embedded = self.onehot_encode(input)\n",
    "            if self.training and self.dropout_input > 0: embedded = self.embedding_dropout(embedded)\n",
    "            embedded = embedded.unsqueeze(0)\n",
    "        else:\n",
    "            embedded = input.unsqueeze(0)\n",
    "            embedded = self.look_up(embedded)\n",
    "\n",
    "        output, hidden = self.gru(embedded, hidden) #(num_layer, B, H)\n",
    "        output = output.view(-1, output.size(-1))  #(B,H)\n",
    "        logit = self.final_activation(self.h2o(output))\n",
    "\n",
    "        return logit, hidden\n",
    "\n",
    "    def init_emb(self):\n",
    "        '''\n",
    "        Initialize the one_hot embedding buffer, which will be used for producing the one-hot embeddings efficiently\n",
    "        '''\n",
    "        onehot_buffer = torch.FloatTensor(self.batch_size, self.output_size)\n",
    "        onehot_buffer = onehot_buffer.to(self.device)\n",
    "        return onehot_buffer\n",
    "\n",
    "    def onehot_encode(self, input):\n",
    "        \"\"\"\n",
    "        Returns a one-hot vector corresponding to the input\n",
    "        Args:\n",
    "            input (B,): torch.LongTensor of item indices\n",
    "            buffer (B,output_size): buffer that stores the one-hot vector\n",
    "        Returns:\n",
    "            one_hot (B,C): torch.FloatTensor of one-hot vectors\n",
    "        \"\"\"\n",
    "        self.onehot_buffer.zero_()\n",
    "        index = input.view(-1, 1)\n",
    "        one_hot = self.onehot_buffer.scatter_(1, index, 1)\n",
    "        return one_hot\n",
    "\n",
    "    def embedding_dropout(self, input):\n",
    "        p_drop = torch.Tensor(input.size(0), 1).fill_(1 - self.dropout_input)\n",
    "        mask = torch.bernoulli(p_drop).expand_as(input) / (1 - self.dropout_input)\n",
    "        mask = mask.to(self.device)\n",
    "        input = input * mask\n",
    "        return input\n",
    "\n",
    "    def init_hidden(self):\n",
    "        '''\n",
    "        Initialize the hidden state of the GRU\n",
    "        '''\n",
    "        try:\n",
    "            h0 = torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(self.device)\n",
    "        except:\n",
    "            self.device = 'cpu'\n",
    "            h0 = torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(self.device)\n",
    "        return h0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-shade",
   "metadata": {},
   "source": [
    "#### Calcul des métriques de performance <a class=\"anchor\" id=\"metrique\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "confident-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall(indices, targets): #recall --> wether next item in session is within top K=20 recommended items or not\n",
    "    \"\"\"\n",
    "    Calculates the recall score for the given predictions and targets\n",
    "    Args:\n",
    "        indices (Bxk): torch.LongTensor. top-k indices predicted by the model.\n",
    "        targets (B): torch.LongTensor. actual target indices.\n",
    "    Returns:\n",
    "        recall (float): the recall score\n",
    "    \"\"\"\n",
    "    targets = targets.view(-1, 1).expand_as(indices)\n",
    "    hits = (targets == indices).nonzero()\n",
    "    if len(hits) == 0:\n",
    "        return 0\n",
    "    n_hits = (targets == indices).nonzero()[:, :-1].size(0)\n",
    "    recall = float(n_hits) / targets.size(0)\n",
    "    return recall\n",
    "\n",
    "\n",
    "def get_mrr(indices, targets): #Mean Receiprocal Rank --> Average of rank of next item in the session.\n",
    "    \"\"\"\n",
    "    Calculates the MRR score for the given predictions and targets\n",
    "    Args:\n",
    "        indices (Bxk): torch.LongTensor. top-k indices predicted by the model.\n",
    "        targets (B): torch.LongTensor. actual target indices.\n",
    "    Returns:\n",
    "        mrr (float): the mrr score\n",
    "    \"\"\"\n",
    "    tmp = targets.view(-1, 1)\n",
    "    targets = tmp.expand_as(indices)\n",
    "    hits = (targets == indices).nonzero()\n",
    "    ranks = hits[:, -1] + 1\n",
    "    ranks = ranks.float()\n",
    "    rranks = torch.reciprocal(ranks)\n",
    "    mrr = torch.sum(rranks).data / targets.size(0)\n",
    "    return mrr.item()\n",
    "\n",
    "\n",
    "def evaluate(indices, targets, k=20):\n",
    "    \"\"\"\n",
    "    Evaluates the model using Recall@K, MRR@K scores.\n",
    "\n",
    "    Args:\n",
    "        logits (B,C): torch.LongTensor. The predicted logit for the next items.\n",
    "        targets (B): torch.LongTensor. actual target indices.\n",
    "\n",
    "    Returns:\n",
    "        recall (float): the recall score\n",
    "        mrr (float): the mrr score\n",
    "    \"\"\"\n",
    "    _, indices = torch.topk(indices, k, -1)\n",
    "    recall = get_recall(indices, targets)\n",
    "    mrr = get_mrr(indices, targets)\n",
    "    return recall, mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-purse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "wound-grove",
   "metadata": {},
   "source": [
    "#### fonction d'entrainement et d'évaluation <a class=\"anchor\" id=\"train_eval\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "negative-resident",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation(object):\n",
    "    \"\"\"Evaluation of network\n",
    "    inputs\n",
    "    --------\n",
    "    model : neural network model using for the task\n",
    "    loss_func: loss function using to compute loss\n",
    "    use_cuda: wether we will set loss on the GPU or not\n",
    "    k: The top k prediction of the network\n",
    "    \n",
    "    returns\n",
    "    -----------\n",
    "    mean_losses, mean_recall,mean_mrr\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, model, loss_func, use_cuda, k=20):\n",
    "        self.model = model\n",
    "        self.loss_func = loss_func\n",
    "        self.topk = k\n",
    "        self.device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "    def eval(self, eval_data, batch_size):\n",
    "        self.model.eval() #set the network on evaluation mode\n",
    "        losses = [] #list of losses over batches\n",
    "        recalls = [] #list of recalls over batches\n",
    "        mrrs = [] #list of mrr over batches\n",
    "        dataloader = DataLoader(eval_data, batch_size) ##load batchs\n",
    "        with torch.no_grad():\n",
    "            hidden = self.model.init_hidden()\n",
    "            for ii, (inputs, target, mask) in tqdm(enumerate(dataloader), total=len(dataloader.dataset.df) // dataloader.batch_size, miniters = 1000):\n",
    "            \n",
    "                inputs = inputs.to(self.device) #set input to GPU\n",
    "                target = target.to(self.device) #Set targets to GPU\n",
    "                logit, hidden = self.model(inputs, hidden)\n",
    "                logit_sampled = logit[:, target.view(-1)]\n",
    "                loss = self.loss_func(logit_sampled)\n",
    "                recall, mrr = evaluate(logit, target, k=self.topk)\n",
    "\n",
    "                # torch.Tensor.item() to get a Python number from a tensor containing a single value\n",
    "                losses.append(loss.item())\n",
    "                recalls.append(recall)\n",
    "                mrrs.append(mrr)\n",
    "        mean_losses = np.mean(losses)\n",
    "        mean_recall = np.mean(recalls)\n",
    "        mean_mrr = np.mean(mrrs)\n",
    "\n",
    "        return mean_losses, mean_recall,mean_mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "indie-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    \"\"\"Class to train the network\n",
    "    \"\"\"\n",
    "    def __init__(self, model, train_data, eval_data, optim, use_cuda, loss_func, batch_size,k_eval):\n",
    "        self.model = model\n",
    "        self.train_data = train_data\n",
    "        self.eval_data = eval_data\n",
    "        self.optim = optim\n",
    "        self.loss_func = loss_func\n",
    "        self.evaluation = Evaluation(self.model, self.loss_func, use_cuda, k = k_eval)\n",
    "        self.device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "        self.batch_size = batch_size\n",
    "        #self.args = args\n",
    "\n",
    "    def train(self, start_epoch, end_epoch, start_time=None):\n",
    "        results=[]\n",
    "        if start_time is None:\n",
    "            self.start_time = time.time()\n",
    "        else:\n",
    "            self.start_time = start_time\n",
    "\n",
    "        for epoch in range(start_epoch, end_epoch + 1):\n",
    "            st = time.time()\n",
    "            print('Start Epoch #', epoch)\n",
    "            train_loss = self.train_epoch(epoch)\n",
    "            loss, recall, mrr = self.evaluation.eval(self.eval_data, self.batch_size)\n",
    "            #loss, recall,mrr= self.evaluation.eval(self.eval_data, self.batch_size)\n",
    "            \n",
    "\n",
    "\n",
    "            print(\"Epoch: {}, train loss: {:.4f}, loss: {:.4f}, recall: {:.4f}, mrr: {:.4f}, time: {}\".format(epoch, train_loss, loss, recall, mrr, time.time() - st))\n",
    "            results.append([train_loss,loss,recall,mrr])\n",
    "        return results\n",
    "            \n",
    "\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "\n",
    "        def reset_hidden(hidden, mask):\n",
    "            \"\"\"Helper function that resets hidden state when some sessions terminate\"\"\"\n",
    "            if len(mask) != 0:\n",
    "                hidden[:, mask, :] = 0\n",
    "            return hidden\n",
    "\n",
    "        hidden = self.model.init_hidden()\n",
    "        dataloader = DataLoader(self.train_data, self.batch_size)\n",
    "        \n",
    "        for ii, (inputs, target, mask) in tqdm(enumerate(dataloader), total=len(dataloader.dataset.df) // dataloader.batch_size, miniters = 1000):\n",
    "            inputs = inputs.to(self.device)\n",
    "            target = target.to(self.device)\n",
    "            self.optim.zero_grad()\n",
    "            hidden = reset_hidden(hidden, mask).detach()\n",
    "            logit, hidden = self.model(inputs, hidden)\n",
    "            # output sampling\n",
    "            logit_sampled = logit[:, target.view(-1)]\n",
    "            loss = self.loss_func(logit_sampled)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "\n",
    "        mean_losses = np.mean(losses)\n",
    "        return mean_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-knowing",
   "metadata": {},
   "source": [
    "#### Entrainements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "union-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "##reading of training data and evaluation data\n",
    "train_data = Dataset('rsc15_train_tr.txt')\n",
    "valid_data = Dataset('rsc15_train_valid.txt', itemmap=train_data.itemmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "clinical-investment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialisation of hyper parameters\n",
    "input_size = len(train_data.items)\n",
    "hidden_size = 100\n",
    "num_layers = 3\n",
    "output_size = input_size\n",
    "batch_size = 32\n",
    "dropout_input = 0\n",
    "dropout_hidden = 0.5\n",
    "embedding_dim = -1\n",
    "final_act = 'tanh'\n",
    "loss_type = 'TOP1-max'\n",
    "optimizer_type = 'Adagrad'\n",
    "lr = 0.01\n",
    "weight_decay = 0\n",
    "momentum = 0\n",
    "eps = 1e-6\n",
    "n_epochs = 2\n",
    "time_sort = False\n",
    "cuda = torch.cuda.is_available()\n",
    "is_eval = False\n",
    "k_eval=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "foreign-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialisation of loss function\n",
    "loss_function = LossFunction(loss_type=loss_type, use_cuda=cuda) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-secretariat",
   "metadata": {},
   "source": [
    "Nous allons entrainer toutes les fonctions de pertes sur trois époques avec les mêmes hyper paramètres et observer les métriques de performance. Le but est d'effectivement valider la théorie des auteurs **les nouvelles fonctions de pertes BPR-max et TOP1-max** ont de meilleurs résultats en terme de recall et de mrr. Le choix du nombres d'époques est dû aux ressources computationnelles à notre disposition. Pour vérifier si les nouvelles fonctions de pertes ne sont pas dépendantes du type d'échantillonage nous avons choisi de garder l'ancienne technique d'échantillonnage (les autres items des batchs réprésentent les échantillons négatifs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "approximate-buffalo",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_type = 'BPR-max'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "american-printing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                       | 0/986843 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### START TRAINING....\n",
      "Start Epoch # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████▊                  | 738282/986843 [2:19:21<46:55, 88.29it/s]\n",
      " 79%|████████████████████████████████████████████████████████████▌                | 1430/1819 [00:04<00:01, 311.79it/s]\n",
      "  0%|                                                                                       | 0/986843 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 0.0281, loss: 0.0289, recall: 0.3100, mrr: 0.0813, time: 8366.553636550903\n",
      "Start Epoch # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████                  | 738282/986843 [1:47:16<36:06, 114.71it/s]\n",
      " 79%|████████████████████████████████████████████████████████████▌                | 1430/1819 [00:03<00:00, 436.11it/s]\n",
      "  0%|                                                                                       | 0/986843 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: 0.0278, loss: 0.0289, recall: 0.3474, mrr: 0.1001, time: 6439.668519258499\n",
      "Start Epoch # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████                  | 738282/986843 [1:38:49<33:16, 124.50it/s]\n",
      " 79%|████████████████████████████████████████████████████████████▌                | 1430/1819 [00:03<00:00, 451.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, train loss: 0.0277, loss: 0.0288, recall: 0.3750, mrr: 0.1126, time: 5933.115788936615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "if not is_eval: #training\n",
    "        #Initialize the model\n",
    "        model = GRU4REC(input_size, hidden_size, output_size, final_act=final_act,\n",
    "                            num_layers=num_layers, use_cuda=cuda, batch_size=batch_size,\n",
    "                            dropout_input=dropout_input, dropout_hidden=dropout_hidden, embedding_dim=embedding_dim).to('cuda')\n",
    "        #weights initialization\n",
    "        #init_model(model)\n",
    "        #optimizer\n",
    "        optimizer = Optimizer(model.parameters(), optimizer_type=optimizer_type, lr=lr,\n",
    "                                  weight_decay=weight_decay, momentum=momentum, eps=eps)\n",
    "        #trainer class\n",
    "        trainer = Trainer(model, train_data=train_data, eval_data=valid_data, optim=optimizer,\n",
    "                              use_cuda=cuda, loss_func=loss_function, batch_size=batch_size,k_eval=k_eval)\n",
    "        print('#### START TRAINING....')\n",
    "        result1 = trainer.train(0, n_epochs - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "based-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"result1.txt\", \"w\") as fp:\n",
    "    json.dump(result1, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "narrative-release",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_type = 'BPR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "injured-battle",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                       | 0/986843 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### START TRAINING....\n",
      "Start Epoch # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████                  | 738282/986843 [1:38:47<33:15, 124.54it/s]\n",
      " 79%|████████████████████████████████████████████████████████████▌                | 1430/1819 [00:03<00:00, 459.26it/s]\n",
      "  0%|                                                                                       | 0/986843 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 0.0281, loss: 0.0289, recall: 0.3415, mrr: 0.0919, time: 5931.1076028347015\n",
      "Start Epoch # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████                  | 738282/986843 [1:38:52<33:17, 124.46it/s]\n",
      " 79%|████████████████████████████████████████████████████████████▌                | 1430/1819 [00:03<00:00, 453.08it/s]\n",
      "  0%|                                                                                       | 0/986843 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: 0.0277, loss: 0.0288, recall: 0.4031, mrr: 0.1197, time: 5935.199015378952\n",
      "Start Epoch # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████                  | 738282/986843 [1:38:49<33:16, 124.51it/s]\n",
      " 79%|████████████████████████████████████████████████████████████▌                | 1430/1819 [00:03<00:00, 443.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, train loss: 0.0277, loss: 0.0287, recall: 0.4217, mrr: 0.1285, time: 5932.992501974106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "if not is_eval: #training\n",
    "        #Initialize the model\n",
    "        model2 = GRU4REC(input_size, hidden_size, output_size, final_act=final_act,\n",
    "                            num_layers=num_layers, use_cuda=cuda, batch_size=batch_size,\n",
    "                            dropout_input=dropout_input, dropout_hidden=dropout_hidden, embedding_dim=embedding_dim).to('cuda')\n",
    "        #weights initialization\n",
    "        #init_model(model)\n",
    "        #optimizer\n",
    "        optimizer = Optimizer(model2.parameters(), optimizer_type=optimizer_type, lr=lr,\n",
    "                                  weight_decay=weight_decay, momentum=momentum, eps=eps)\n",
    "        #trainer class\n",
    "        trainer = Trainer(model2, train_data=train_data, eval_data=valid_data, optim=optimizer,\n",
    "                              use_cuda=cuda, loss_func=loss_function, batch_size=batch_size,k_eval=k_eval)\n",
    "        print('#### START TRAINING....')\n",
    "        result2 = trainer.train(0, n_epochs - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "sealed-steal",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"result2.txt\", \"w\") as fp:\n",
    "    json.dump(result2, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "round-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_type = 'TOP1-max'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "incorporated-ethiopia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                       | 0/986843 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### START TRAINING....\n",
      "Start Epoch # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████                  | 738282/986843 [1:38:48<33:16, 124.53it/s]\n",
      " 79%|████████████████████████████████████████████████████████████▌                | 1430/1819 [00:03<00:00, 441.21it/s]\n",
      "  0%|                                                                                       | 0/986843 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 0.0281, loss: 0.0288, recall: 0.3228, mrr: 0.0922, time: 5932.036278963089\n",
      "Start Epoch # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████                  | 738282/986843 [1:38:57<33:18, 124.35it/s]\n",
      " 79%|████████████████████████████████████████████████████████████▌                | 1430/1819 [00:03<00:00, 451.13it/s]\n",
      "  0%|                                                                                       | 0/986843 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: 0.0277, loss: 0.0286, recall: 0.3881, mrr: 0.1182, time: 5940.254254579544\n",
      "Start Epoch # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████                  | 738282/986843 [1:39:30<33:30, 123.65it/s]\n",
      " 79%|████████████████████████████████████████████████████████████▌                | 1430/1819 [00:03<00:00, 441.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, train loss: 0.0276, loss: 0.0286, recall: 0.4025, mrr: 0.1227, time: 5973.8704698085785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "if not is_eval: #training\n",
    "        #Initialize the model\n",
    "        model3 = GRU4REC(input_size, hidden_size, output_size, final_act=final_act,\n",
    "                            num_layers=num_layers, use_cuda=cuda, batch_size=batch_size,\n",
    "                            dropout_input=dropout_input, dropout_hidden=dropout_hidden, embedding_dim=embedding_dim).to('cuda')\n",
    "        #weights initialization\n",
    "        #init_model(model)\n",
    "        #optimizer\n",
    "        optimizer = Optimizer(model3.parameters(), optimizer_type=optimizer_type, lr=lr,\n",
    "                                  weight_decay=weight_decay, momentum=momentum, eps=eps)\n",
    "        #trainer class\n",
    "        trainer = Trainer(model3, train_data=train_data, eval_data=valid_data, optim=optimizer,\n",
    "                              use_cuda=cuda, loss_func=loss_function, batch_size=batch_size,k_eval=k_eval)\n",
    "        print('#### START TRAINING....')\n",
    "        result3 = trainer.train(0, n_epochs - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "secure-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"result3.txt\", \"w\") as fp:\n",
    "    json.dump(result3, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "sensitive-union",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_type = 'TOP1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "waiting-conditions",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                       | 0/986843 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### START TRAINING....\n",
      "Start Epoch # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████                  | 738282/986843 [1:38:56<33:18, 124.36it/s]\n",
      " 79%|████████████████████████████████████████████████████████████▌                | 1430/1819 [00:03<00:00, 439.04it/s]\n",
      "  0%|                                                                                       | 0/986843 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 0.0281, loss: 0.0288, recall: 0.3520, mrr: 0.0997, time: 5939.731620788574\n",
      "Start Epoch # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████                  | 738282/986843 [1:39:01<33:20, 124.25it/s]\n",
      " 79%|████████████████████████████████████████████████████████████▌                | 1430/1819 [00:03<00:00, 434.06it/s]\n",
      "  0%|                                                                                       | 0/986843 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: 0.0277, loss: 0.0287, recall: 0.4015, mrr: 0.1248, time: 5945.074543714523\n",
      "Start Epoch # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████                  | 738282/986843 [1:39:15<33:25, 123.96it/s]\n",
      " 79%|████████████████████████████████████████████████████████████▌                | 1430/1819 [00:03<00:00, 443.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, train loss: 0.0277, loss: 0.0287, recall: 0.4101, mrr: 0.1254, time: 5958.955616474152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "if not is_eval: #training\n",
    "        #Initialize the model\n",
    "        model4 = GRU4REC(input_size, hidden_size, output_size, final_act=final_act,\n",
    "                            num_layers=num_layers, use_cuda=cuda, batch_size=batch_size,\n",
    "                            dropout_input=dropout_input, dropout_hidden=dropout_hidden, embedding_dim=embedding_dim).to('cuda')\n",
    "        #weights initialization\n",
    "        #init_model(model)\n",
    "        #optimizer\n",
    "        optimizer = Optimizer(model4.parameters(), optimizer_type=optimizer_type, lr=lr,\n",
    "                                  weight_decay=weight_decay, momentum=momentum, eps=eps)\n",
    "        #trainer class\n",
    "        trainer = Trainer(model4, train_data=train_data, eval_data=valid_data, optim=optimizer,\n",
    "                              use_cuda=cuda, loss_func=loss_function, batch_size=batch_size,k_eval=k_eval)\n",
    "        print('#### START TRAINING....')\n",
    "        result4 = trainer.train(0, n_epochs - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "legal-swift",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"result4.txt\", \"w\") as fp:\n",
    "    json.dump(result4, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "colonial-strain",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_type = 'SampledCrossEntropyLoss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "endless-creation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                       | 0/986843 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### START TRAINING....\n",
      "Start Epoch # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████                  | 738282/986843 [1:40:41<33:53, 122.21it/s]\n",
      " 79%|████████████████████████████████████████████████████████████▌                | 1430/1819 [00:03<00:00, 452.45it/s]\n",
      "  0%|                                                                                       | 0/986843 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 0.0281, loss: 0.0288, recall: 0.3058, mrr: 0.0765, time: 6044.509494781494\n",
      "Start Epoch # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████                  | 738282/986843 [1:38:55<33:18, 124.39it/s]\n",
      " 79%|████████████████████████████████████████████████████████████▌                | 1430/1819 [00:03<00:00, 434.72it/s]\n",
      "  0%|                                                                                       | 0/986843 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: 0.0278, loss: 0.0287, recall: 0.3541, mrr: 0.0957, time: 5938.782986402512\n",
      "Start Epoch # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████                  | 738282/986843 [1:39:27<33:29, 123.71it/s]\n",
      " 79%|████████████████████████████████████████████████████████████▌                | 1430/1819 [00:03<00:00, 444.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, train loss: 0.0277, loss: 0.0287, recall: 0.3663, mrr: 0.1012, time: 5971.099236011505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "if not is_eval: #training\n",
    "        #Initialize the model\n",
    "        model5 = GRU4REC(input_size, hidden_size, output_size, final_act=final_act,\n",
    "                            num_layers=num_layers, use_cuda=cuda, batch_size=batch_size,\n",
    "                            dropout_input=dropout_input, dropout_hidden=dropout_hidden, embedding_dim=embedding_dim).to('cuda')\n",
    "        #weights initialization\n",
    "        #init_model(model)\n",
    "        #optimizer\n",
    "        optimizer = Optimizer(model5.parameters(), optimizer_type=optimizer_type, lr=lr,\n",
    "                                  weight_decay=weight_decay, momentum=momentum, eps=eps)\n",
    "        #trainer class\n",
    "        trainer = Trainer(model5, train_data=train_data, eval_data=valid_data, optim=optimizer,\n",
    "                              use_cuda=cuda, loss_func=loss_function, batch_size=batch_size,k_eval=k_eval) \n",
    "        print('#### START TRAINING....')\n",
    "        result5 = trainer.train(0, n_epochs - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "compatible-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"result5.txt\", \"w\") as fp:\n",
    "    json.dump(result5, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "brutal-douglas",
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls =[result1[2][2]*100,result2[2][2]*100,result3[2][2]*100,result4[2][2]*100,result5[2][2]*100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "compliant-exhaust",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEXCAYAAACqIS9uAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAfH0lEQVR4nO3deZwcVb338c+XECEsASIjhiWJRgRZgwzoVdGwyb75CJKLEF4qwUfRq6KyPDwsKle8V0RQrxgWE0CWyC6LkBsNAfECiWSDoJEQZAnJsBk2MSS/+8c5QzqdnpmeyVR3JvV9v179mqpTy/lVdc+vq05Vn1JEYGZm5bFWswMwM7PGcuI3MysZJ34zs5Jx4jczKxknfjOzknHiNzMrGSd+awhJkyV9IQ8fL+m+HqxjM0lTJL0i6fwCYhwmKSStncfvlDS6Yvr3JD0v6bk8foSkpyS9KmmX3o6nKJXvhZXT2s0OwKwbxgDPAwOjAT9AiYgD2oclbQWcDAyNiEW5+IfASRFxS9GxVJM0GbgqIi5tdN3W9/mI397WfqS7GhsKPNqTpN8L2zYUeKEi6beXPdKTlfWBfW1rMCf+kpM0X9IpkmYCr0laW9KHJd0v6WVJMySNrJh/kKRfSnpW0kuSbs7lm0i6TVJbLr9N0pZ11C9JF0haJOnvkmZK2qHGfOOA0cC3c9PKPpLWkfTjHMuzeXidPP9ISU/nbXsO+GWNdfaT9MPcfDMPOKhq+mRJX5C0DzAR2DzXfY2kV4F+wAxJj+f5N5d0Q94HT0j6asW6zpZ0vaSrJC0Gjpe0kaTLJC2Q9ExuSuqX5z9e0n05vpfy+g7I084F9gB+muP5ab37scY+WEvSGZKezMteIWmjPG3dHO8L+bPwkKTNKuKbl5vdnpB0TMU6PydpTo77LklDu/NeWwNEhF8lfgHzgenAVsAAYAvgBeBA0oHBvnm8Jc9/O3AdsAnQH/hELn8n8H+A9YANgV8DN1fUMxn4Qh4+HrgvD+8HTAM2BgR8ABjcQazjgO9VjH8H+B/gXUALcD/w3TxtJPAW8ANgHWBAjfV9EXgsb/sg4PdAAGvXiHkk8HTV8gG8Lw+vlbfjTOAdwHuBecB+efrZwBLg8DzvAOBm4BfA+nkbHgROrNhHS4ATSF8w/xd4FlB1bD3Yj5Xb9TngrzneDYAbgSvztBOB3+T3tB+wKzAwx7sY2CbPNxjYPg8fntf3AVJT8hnA/d2N0a9iXz7iN4CLIuKpiHgD+CxwR0TcERHLImIiMBU4UNJg4ADgixHxUkQsiYh7ACLihYi4ISJej4hXgHOBT9RR9xLSF8W2pKQ2JyIW1Bn3McB3ImJRRLQB5wDHVkxfBpwVEW/mbat2FPDjvO0vAt+vs95adiN9OX4nIv4ZEfOAS4CjK+b5Y0TcHBHLSAn0AOBrEfFapCakC6rmfzIiLomIpcB4UoLdrIP6e7ofjwF+FBHzIuJV4DTg6NwUtYT0hf6+iFgaEdMiYnFebhmwg6QBEbEgItqbvE4Evp/rfwv4d2BEPupflffaepETvwE8VTE8FDgyn9q/LOll4GOkpLMV8GJEvFS9AknrSfpFbjJYDEwBNm5vuuhIRPwO+CnwM2ChpLGSBtYZ9+bAkxXjT+aydm0R8Y8ulq/c9ic7mrEOQ0lNQZX77XRWTNTV+7k/sKBi/l+QjvzbPdc+EBGv58ENalW+Cvux1j5cO8d9JXAXcG1uSvsPSf0j4jXgM6QzpgWSbpe0bcV2XVixTS+Sju63WMX32nqRE79BarJo9xTpVH/jitf6EXFenjZI0sY11nEysA3woYgYCHw8l6vLyiMuiohdge2B9wPfqjPuZ0mJpt2QXFZru2pZQPoyq1y+p54CnqjabxtGxIEdxPMU8CawacX8AyNi+zrrW2nbergfa+3Dt4CF+YzunIjYDvgIcDBwXK7rrojYl3RA8Bjp7KZ9u06s2g8DIuL+VYjRepkTv1W7CjhE0n754ue6+ULplvm0/E7gv5Qu5vaX1J7gNwTeAF6WNAg4q57KJO0m6UOS+gOvAf8AltYZ6zXAGZJaJG1Kal+/qv5NZQLwVUlbStoEOLUby1Z7EFisdDF5QN53O0jardbMeV/eDZwvaWC+yDpcUj3NYwALSe3ywCrtx2uAr0t6j6QNSE0z10XEW5L2lLRjPmtbTGqqWar0e4pDJa1P+vJ6taKui4HTJG2f49pI0pGrGKP1Mid+W0FEPAUcRmqmaCMdwX2L5Z+VY0kJ4DFgEfC1XP5j0gXL50kXXH9bZ5UDSUeLL5GaGV4g3R9fj++Rrj/MBGYBf8pl9bqE1JQxIy97YzeWXUFuhz8EGAE8QdoPlwIbdbLYcaQLwY+Stv960hF0PS4EPp3vnLmInu/Hy0lNOlNy3P8AvpKnvTvHtBiYA9xD+mJdi3SG9yypKecTwJcAIuIm0gX1a3OT32zStQxWIUbrZe13CJiZWUn4iN/MrGSc+M3MSsaJ38ysZJz4zcxKpk90FLXpppvGsGHDmh2GmVmfMm3atOcjoqW6vE8k/mHDhjF16tRmh2Fm1qdIqvlrdDf1mJmVjBO/mVnJOPGbmZWME7+ZWck48ZuZlYwTv5lZyTjxm5mVjBO/mVnJOPGbmZVMn/jlrvXcsFNvb3YIvWL+eQc1OwSzNYaP+M3MSsaJ38ysZJz4zcxKxonfzKxknPjNzErGd/WYrYHWlLu5wHd0FaHwI35J/SQ9LOm2PD5I0kRJc/PfTYqOwczMlmtEU8+/AXMqxk8FJkXE1sCkPG5mZg1SaFOPpC2Bg4BzgW/k4sOAkXl4PDAZOKXIOKyc3NxhVlvRR/w/Br4NLKso2ywiFgDkv++qtaCkMZKmSpra1tZWcJhmZuVRWOKXdDCwKCKm9WT5iBgbEa0R0drSstJD4s3MrIeKbOr5KHCopAOBdYGBkq4CFkoaHBELJA0GFhUYg5mZVSnsiD8iTouILSNiGHA08LuI+CxwKzA6zzYauKWoGMzMbGXN+AHXecC+kuYC++ZxMzNrkIb8gCsiJpPu3iEiXgD2bkS9Zma2MnfZYGZWMk78ZmYl48RvZlYyTvxmZiXjxG9mVjJO/GZmJePEb2ZWMmv8g1jcQ6OZ2Yp8xG9mVjJO/GZmJePEb2ZWMmt8G7+ZlY+v7XXOR/xmZiXjxG9mVjJO/GZmJePEb2ZWMkU+bH1dSQ9KmiHpEUnn5PKzJT0jaXp+HVhUDGZmtrIi7+p5E9grIl6V1B+4T9KdedoFEfHDAus2M7MOFJb4IyKAV/No//yKouozM7P6FNrGL6mfpOnAImBiRDyQJ50kaaakyyVt0sGyYyRNlTS1ra2tyDDNzEql0MQfEUsjYgSwJbC7pB2AnwPDgRHAAuD8DpYdGxGtEdHa0tJSZJhmZqXSkLt6IuJlYDKwf0QszF8Iy4BLgN0bEYOZmSVF3tXTImnjPDwA2Ad4TNLgitmOAGYXFYOZma2syLt6BgPjJfUjfcFMiIjbJF0paQTpQu984MQCYzAzsypF3tUzE9ilRvmxRdVpZmZd8y93zcxKxonfzKxknPjNzErGid/MrGSc+M3MSsaJ38ysZJz4zcxKxonfzKxknPjNzErGid/MrGSc+M3MSsaJ38ysZJz4zcxKxonfzKxknPjNzEqmyCdwrSvpQUkzJD0i6ZxcPkjSRElz89+aD1s3M7NiFHnE/yawV0TsTHqw+v6SPgycCkyKiK2BSXnczMwapLDEH8mrebR/fgVwGDA+l48HDi8qBjMzW1mhbfyS+kmaDiwCJkbEA8BmEbEAIP99V5ExmJnZigpN/BGxNCJGAFsCu0vaod5lJY2RNFXS1La2tsJiNDMrm4bc1RMRLwOTgf2BhZIGA+S/izpYZmxEtEZEa0tLSyPCNDMrhSLv6mmRtHEeHgDsAzwG3AqMzrONBm4pKgYzM1vZ2gWuezAwXlI/0hfMhIi4TdIfgQmSPg/8DTiywBjMzKxKYYk/ImYCu9QofwHYu6h6zcysc/7lrplZyTjxm5mVjBO/mVnJOPGbmZVMtxK/pOGSdiwqGDMzK17dd/VIOh3YEVgmaVlEHFtcWGZmVpQOj/glfSXfg99u54gYFRHHADsXH5qZmRWhs6ael4DfSjokj98t6R5J9wJ3FR+amZkVocPEHxFXAYcAIyTdAkwFDgAOjohvNSg+MzPrZV1d3B0OXAecCJwE/BgYUHBMZmZWoA4v7koal6cPAB6PiBMk7QJcIunBiPhug2I0M7Ne1NldPbvkxyYi6WGAiHgYOETSYY0IzszMel9nif9OSfcA7wCurpwQEe5K2cysj+ow8UfEqZIGAssqnp1rZmZ9XKc/4IqIxY0KxMzMGsN99ZiZlUyRj17cStLvJc2R9Iikf8vlZ0t6RtL0/DqwqBjMzGxldfXVI+kjwLDK+SPiii4Wews4OSL+JGlDYJqkiXnaBRHxwx7Ea2Zmq6jLxC/pStIPuaYDS3NxAJ0m/ohYACzIw69ImgNssSrBmpnZqqvniL8V2C4ioqeVSBpGev7uA8BHgZMkHUfqBuLkiHipxjJjgDEAQ4YM6WnVZmZWpZ42/tnAu3tagaQNgBuAr+W7hH5OOoMYQTojOL/WchExNiJaI6K1paWlp9WbmVmVeo74NwUelfQg8GZ7YUQc2tWCkvqTkv6vIuLGvNzCiumXALd1N2gzM+u5ehL/2T1ZsSQBlwFzIuJHFeWDc/s/wBGkMwozM2uQLhN/RNwjaSiwdUT8t6T1gH5dLUdqyz8WmCVpei47HRglaQTpAvF8Us+fZmbWIPXc1XMC6SLrIFLb/BbAxcDenS0XEfcBqjHpju6HaWZmvaWei7tfJh29LwaIiLnAu4oMyszMilNP4n8zIv7ZPiJpbVIzjZmZ9UH1JP57JJ0ODJC0L/Br4DfFhmVmZkWpJ/GfCrQBs0gXYu8AzigyKDMzK049d/UsAy7JLzMz6+PcLbOZWck48ZuZlUy3Er+ktfLjGM3MrI/qMvFLulrSQEnrA48Cf5b0reJDMzOzItRzxL9d7lXzcNIdPUNIXTGYmVkfVE/i75972TwcuCUiluAfcJmZ9Vn1JP5fkDpTWx+YkjtsW1xkUGZmVpx67uO/CLioouhJSXsWF5KZmRWpw8Qv6RtdLPujLqabmdlqqLMj/g0bFoWZmTVMh4k/Is5ZlRVL2gq4gvS83mXA2Ii4UNIg4DpgGOnawVG1HrZuZmbF6Kyp56KOpgFExFe7WPdbwMkR8SdJGwLTJE0EjgcmRcR5kk4ldQJ3SvfCNjOznuqsqWfaqqw4P1d3QR5+RdIc0tO7DgNG5tnGA5Nx4jcza5jOmnrG91YlkoYBuwAPAJu1P2w9IhZI8tO8zMwaqJ5n7raQjsi3A9ZtL4+IveqpQNIGwA3A1yJisVTrMbw1lxtDetYvQ4YMqWsZMzPrWj0/4PoVMAd4D3AO6YLsQ/WsPP/i9wbgVxFxYy5eKGlwnj4YWFRr2YgYGxGtEdHa0tJST3VmZlaHehL/OyPiMmBJRNwTEZ8DPtzVQkqH9pcBcyKi8p7/W4HReXg0cEs3YzYzs1XQZVMPsCT/XSDpIOBZYMs6lvsoqTO3WZKm57LTgfOACZI+D/wNOLJbEZuZ2SqpJ/F/T9JGwMnAT4CBwNe7Wigi7gM6atDfu+4IzcysV9XTV89tefDvgPvoMTPr4+p5EMt4SRtXjG8i6fJCozIzs8LUc3F3p4h4uX0kd6+wS2ERmZlZoepJ/GtJ2qR9JPe1U8+1ATMzWw3Vk8DPB+6XdD3pyVtHAecWGpWZmRWmnou7V0iaCuxFukvnUxHxaOGRmZlZIepp6gEYBLwWET8B2iS9p8CYzMysQPXc1XMWqa+e03JRf+CqIoMyM7Pi1HPEfwRwKPAaQEQ8i5/OZWbWZ9WT+P8ZEUG6sIuk9YsNyczMilRP4p8g6RfAxpJOAP4buKTYsMzMrCid3tWTe9i8DtgWWAxsA5wZERMbEJuZmRWg08QfESHp5ojYFXCyNzNbA9TT1PM/knYrPBIzM2uIen65uydwoqQnSXf2iHQysFOhkZmZWSHqSfwHFB6FmZk1TJdNPRHxZK1XV8tJulzSIkmzK8rOlvSMpOn5deCqboCZmXVPvV029MQ4YP8a5RdExIj8uqPA+s3MrIbCEn9ETAFeLGr9ZmbWM0Ue8XfkJEkzc1PQJh3NJGmMpKmSpra1tTUyPjOzNVqjE//PgeHACGABqa//miJibES0RkRrS0tLg8IzM1vzNTTxR8TCiFgaEctI3T7s3sj6zcyswYlf0uCK0SOA2R3Na2ZmxSjs2bmSrgFGAptKeho4CxgpaQSpp8/5wIlF1W9mZrUVlvgjYlSN4suKqs/MzOrTjLt6zMysiZz4zcxKxonfzKxknPjNzErGid/MrGSc+M3MSsaJ38ysZJz4zcxKxonfzKxknPjNzErGid/MrGSc+M3MSsaJ38ysZJz4zcxKxonfzKxknPjNzEqmsMQv6XJJiyTNrigbJGmipLn57yZF1W9mZrUVecQ/Dti/quxUYFJEbA1MyuNmZtZAhSX+iJgCvFhVfBgwPg+PBw4vqn4zM6ut0W38m0XEAoD8910dzShpjKSpkqa2tbU1LEAzszXdantxNyLGRkRrRLS2tLQ0OxwzszVGoxP/QkmDAfLfRQ2u38ys9Bqd+G8FRufh0cAtDa7fzKz0iryd8xrgj8A2kp6W9HngPGBfSXOBffO4mZk10NpFrTgiRnUwae+i6jQzs66tthd3zcysGE78ZmYl48RvZlYyTvxmZiXjxG9mVjJO/GZmJePEb2ZWMk78ZmYl48RvZlYyTvxmZiXjxG9mVjJO/GZmJePEb2ZWMk78ZmYl48RvZlYyhfXH3xlJ84FXgKXAWxHR2ow4zMzKqCmJP9szIp5vYv1mZqXkph4zs5JpVuIP4G5J0ySNqTWDpDGSpkqa2tbW1uDwzMzWXM1K/B+NiA8CBwBflvTx6hkiYmxEtEZEa0tLS+MjNDNbQzUl8UfEs/nvIuAmYPdmxGFmVkYNT/yS1pe0Yfsw8ElgdqPjMDMrq2bc1bMZcJOk9vqvjojfNiEOM7NSanjij4h5wM6NrtfMzBLfzmlmVjJO/GZmJePEb2ZWMk78ZmYl48RvZlYyTvxmZiXjxG9mVjJO/GZmJePEb2ZWMk78ZmYl48RvZlYyTvxmZiXjxG9mVjJO/GZmJePEb2ZWMk78ZmYl05TEL2l/SX+W9FdJpzYjBjOzsmrGM3f7AT8DDgC2A0ZJ2q7RcZiZlVUzjvh3B/4aEfMi4p/AtcBhTYjDzKyUFBGNrVD6NLB/RHwhjx8LfCgiTqqabwwwJo9uA/y5oYF236bA880Ookm87eVV5u3vC9s+NCJaqgsb/rB1QDXKVvr2iYixwNjiw+kdkqZGRGuz42gGb3s5tx3Kvf19edub0dTzNLBVxfiWwLNNiMPMrJSakfgfAraW9B5J7wCOBm5tQhxmZqXU8KaeiHhL0knAXUA/4PKIeKTRcRSgzzRLFcDbXl5l3v4+u+0Nv7hrZmbN5V/umpmVjBO/mVnJOPFbpyQtlTRd0gxJf5L0kVw+TNIbedqjki6WtFaN8isk9W/2dljXJL0zv2/TJT0n6ZmK8SGSbpE0V9Ljki7MN2cgaaSkv0t6WNIcSWdVrO/3kl6V9NPmbp1VKlXi70YSeztZdfSh7mD9IyWFpM9XlO2Sy75ZUba2pOclfb+irJ+kaZI+XlF2t6Qje3s/dNMbETEiInYGTgO+XzHt8YgYAexE6n7j8KryHUm36x7VsGizZicxSfMl3VtVNl3S7KqyC3Nsa1WUfUPSZRXjx0i6fVX3SVci4oX8Xo8ALgYuyMO7ANcDN0fE1sD7gQ2AcysWvzcidgFagc9K2hX4B/D/gW/SiyS9W9K1+b17VNIdkt7fm3X0FkkjJB3Y7DiqlSrxU18Sq5Wsan2oOzIL+EzF+NHAjKp5Pkn6JfJRkgQQEUuBLwE/k9Rf0qhUHL/u7kYWaCDwUnVhRLwF3A+8r6p8KfAgsEVDolux7tUhiW0oaSsASR+onpiT/RHAU8DHKyZdBOwq6aOSNga+B3ylG/X2tr2Af0TEL+Ht9/XrwOckrVc5Y0S8BkwDhkfEaxFxH2nf9Yr8/3ITMDkihkfEdsDpwGYV8/Trrfp6wQigZuKX1Iwf0ALlS/yVOkpiHSaryg91J+v9G7CupM3yh3R/4M6qeUYBF+Z5P1yx/gdICfRs4N+BL9e/OYUZkI9UHwMuBb5bPUP+59+b9KVXWb4u8CHgt40ItE6NTGITWH4QMAq4pmr6nsBs4Od5enu9b5EPAoD/IN3yPK8b9fa27Un74W0RsZj0+V3hy17SO0mf6aJu0d4TWBIRF1fEMh3ol8/IrgZmSVpX0i8lzcpnb3vm+LaX9GD+TM+UtLWk9SXdnlsCZkv6TO2qQdKuku7JZ+d3SRqcyydL+kFe918k7ZHPIr8DfCbX9xlJZ0saK+lu4ApJQyVNyrFMkjQkr2+cUvPpvXl9B+fyeyWNqIjnD5J26u5OLFviryeJdZisuvGhvh44EvgI8CfgzYp1DCAlydtIiWBU1bKnAV8Dro6Iv9a1VcVqP0valvQldkX7WQowXNJ04A/A7RFxZ1X5C8DfImJmo4PuRCOT2PXAp/LwIcBvqqa3fxncBBysimshEXE/MAfYh5T8m0nU6FalqnwPSQ8DdwPnFfjbnB2oev8q7A78v3wW8GWAiNiRtJ/H5//tLwIX5rO/VlJPAvsDz0bEzhGxAx0cqOT35yfApyNiV+ByVjxTXDsidif9/56VO6E8E7gu/w9dl+fbFTgsIv4V+ClwRUTsBPyKdLbXbhjwCeAg4OIc/6XA8Tme9wPr9OT/q2yJv54kVitZdfdDPYGU+Gsd5R0M/D4iXgduAI6oOjX9OPB30gd8tRIRfyR1TNXe6dPjeX/uEhFnV8za3mz2PuDDkg5tbKSdamQSexF4SdLRpCT++tuVpaPBA0lNTouBB0hNgO3TNyAlpv4s39/N8kiO5W2SBpK6Xnk8F92bPwe7Vh6NN9iDEfFEHv4YcCVARDwGPElq1vsjcLqkU0gdmL1BOlPdJx+x7xERf+9g/duQ/i8n5lxxBqlZuN2N+e80UtLuyK25XoB/Aa7Ow1fmuNtNiIhlETEXmAdsC/ya5QcJnwPGdVJPh8qW+N/WURKjdrLq1oc6Ip4DlgD7ApOqJo8ifcjmkz4g7ySdviJpfdLR3V5Ay+p2UUjStqRfW79Qz/wRsQA4lXQWs7podBK7jtRkU30AsD+wEalZYj7pH77y7O8c4CrSEeUFqxjDqpoErCfpOHi7Df18YFw+gGmkR0hHzLW8VjFcqzNIIuJq4FDgDeAuSXtFxF/yOmcB35d0ZgfrF/BI+7WjiNgxIj5ZMb39zH4pnfeK8Fon06KD4Rx+vA5MJHVlfxTLvzS6pbSJv6Mk1ovJ6kzglNyG3F7nQNI/+JCIGBYRw0inpKMqlpmQj1C+BFyQT++aqb15bDopiY2u3KY63ExKGnsUEVwPNDqJ3UT6Mr+rqnwU8IWKz8F7gE9KWk/SjqTT+x+QugUYKmnfAmKrS6Sf9x8BHClpLvAX0rWO07taNn+p/Qg4XtLTWvWHLv0OWEfSCRV17EZqEqk0BTgmT38/MAT4s6T3AvMi4iJSH2E7SdoceD0irgJ+CHywg7r/TDog+5e83v6Stu8i3leADTuZfj/pBhByvPdVTDtS6Rbp4cB7Wd41/aWkJqGHIuLFLuqvLSJK8yJ9E0/PrxnAQbl8GDC7Yj7l6XsAI4Hb6lx/zXlJF2u/SWqbu7Zq2iCgDdiZ9A81oGLaRaS2wqbvu778at//FeNbkdrb55KO8n9Caivt8D3M0+aTmm9eJbUNb9dJnfOBTavKhpEu5q6X1zOwavqNpIvB9wEHVJS3Ao8C72j2vlwdXsDmpObUx0lnALcDJ1S+b8C6pGaQWcDDwJ65/LS8zHRSW/4gYD9gZi57CGjtpO4RpC+VGXk9J+Tyye3LkVoS5ufhQXmd0/N7W/1ZHEb6MptJOigZksvHkc707s154eCqOB4jPdekR/vQffWYma1mJI0jfZFdX2Pa5qQvmm0jYllP1l/aph4zs74mN1E+QLp7qUdJH9w7Z49I2o/U/lrpiYg4ohnxWPNIegBYp6r42IiYVWt+6xsk3US67lLplIiovlbTJznxm5mVjJt6zMxKxonfzKxknPjNAEmvNjsGs0Zx4jczKxknfrMKSv4z99I4q72nRkmDJU3Jv2KenXtf7Jd7UWyf9+t53uGSfpt7cLw3/0ocSUfmeWdImtLM7bRya1p/0GarqU+Rfp25M+kXmA/lJP2vwF0RcW7u5mG9PN8WkXp0RKnvfEjdLHwxIuZK+hDwX6T+l84E9ouIZyrmNWs4J36zFX0MuCZSf0QLJd0D7Eb62f3luVfEmyNiuqR5wHsl/YTUbcDduVfNjwC/Xt7x69v3+f8BGCdpAst7cjRrODf1mK2oo14dp5C6zH4GuFLScRHxEunMYDKps71LSf9TL8fyHhxHRMQH8jq+SOrKdytgeu7v36zhnPjNVjSF9MSkfpJaSMn+QUlDgUURcQlwGfBBSZsCa0XEDaTHMn4wUt/6Tyg/KzlfM9g5Dw+PiAci4kzgedIXgFnDuanHbEU3kR6OMYPUH/q3I+I5SaOBb0laQuqd8zjS4zl/qeUPSm/vyvsY4OeSziA9SOXavL7/lLQ16axiEis/i9msIdxlg5lZybipx8ysZJz4zcxKxonfzKxknPjNzErGid/MrGSc+M3MSsaJ38ysZP4XQMSWbgiMYfMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_names = ['BRP_MAX', 'BPR', 'TOP1_MAX', 'TOP1', 'Cross_entropy']\n",
    "plt.bar(loss_names,recalls)\n",
    "plt.xlabel('losses')\n",
    "plt.ylabel('recalls en %')\n",
    "plt.title('recalls for differents losses')\n",
    "plt.savefig('recalls')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-credit",
   "metadata": {},
   "outputs": [],
   "source": [
    "MRRs =[result1[2][3]*100,result2[2][3]*100,result3[2][3]*100,result4[2][3]*100,result5[2][3]*100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-niger",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_names = ['BRP_MAX', 'BPR', 'TOP1_MAX', 'TOP1', 'Cross_entropy']\n",
    "plt.bar(loss_names,MRRs)\n",
    "plt.xlabel('losses')\n",
    "plt.ylabel('MRRs x 1E-1')\n",
    "plt.title('MRRs for differents losses')\n",
    "plt.savefig('MRR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-length",
   "metadata": {},
   "source": [
    "<img src=\"MRR.png\"/>\n",
    "Nous obtenons des résultas contraires à ceux des auteurs, les pertes BPR-max, TOP1-MAX ont des recall et des MRR inférieurs à ceux des TOP1 et BPR même si les différences ne sont pas très élévées. ceci nous permet de conclure que les nouvelles fonctions construites par les auteurs sont dépendentes du style d'échantillonnage.\n",
    "\n",
    "Ensuite nous allons entrainé à nouveau le modèle sur des batchs de taille 500 et 1000 sur 2 époques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "reported-criticism",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ranking-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_type = 'BPR-max'\n",
    "final_act = 'tanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cathedral-river",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/63158 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### START TRAINING....\n",
      "Start Epoch # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████████████████████████████████▊                   | 47245/63158 [17:14<05:48, 45.67it/s]\n",
      "  0%|                                                                                          | 0/116 [00:00<?, ?it/s]<ipython-input-7-c5f494c83781>:11: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:882.)\n",
      "  hits = (targets == indices).nonzero()\n",
      " 75%|████████████████████████████████████████████████████████████▊                    | 87/116 [00:01<00:00, 79.55it/s]\n",
      "  0%|                                                                                        | 0/63158 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 0.0018, loss: 0.0018, recall: 0.3409, mrr: 0.1048, time: 1035.6925649642944\n",
      "Start Epoch # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████████████████████████████████▊                   | 47245/63158 [17:16<05:49, 45.57it/s]\n",
      " 75%|████████████████████████████████████████████████████████████▊                    | 87/116 [00:01<00:00, 77.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: 0.0018, loss: 0.0019, recall: 0.2916, mrr: 0.0792, time: 1037.907169342041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2\n",
    "if not is_eval: #training\n",
    "        #Initialize the model\n",
    "        model = GRU4REC(input_size, hidden_size, output_size, final_act=final_act,\n",
    "                            num_layers=num_layers, use_cuda=cuda, batch_size=batch_size,\n",
    "                            dropout_input=dropout_input, dropout_hidden=dropout_hidden, embedding_dim=embedding_dim).to('cuda')\n",
    "        #weights initialization\n",
    "        #init_model(model)\n",
    "        #optimizer\n",
    "        optimizer = Optimizer(model.parameters(), optimizer_type=optimizer_type, lr=lr,\n",
    "                                  weight_decay=weight_decay, momentum=momentum, eps=eps)\n",
    "        #trainer class\n",
    "        trainer = Trainer(model, train_data=train_data, eval_data=valid_data, optim=optimizer,\n",
    "                              use_cuda=cuda, loss_func=loss_function, batch_size=batch_size,k_eval=k_eval)\n",
    "        print('#### START TRAINING....')\n",
    "        result1 = trainer.train(0, n_epochs - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "chubby-deployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "domestic-straight",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/31579 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### START TRAINING....\n",
      "Start Epoch # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████████████████████████████████▊                   | 23620/31579 [15:39<05:16, 25.13it/s]\n",
      " 71%|█████████████████████████████████████████████████████████▉                        | 41/58 [00:00<00:00, 42.40it/s]\n",
      "  0%|                                                                                        | 0/31579 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 0.0009, loss: 0.0009, recall: 0.3512, mrr: 0.0981, time: 940.8622348308563\n",
      "Start Epoch # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████████████████████████████████▊                   | 23620/31579 [15:40<05:16, 25.11it/s]\n",
      " 71%|█████████████████████████████████████████████████████████▉                        | 41/58 [00:00<00:00, 42.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: 0.0009, loss: 0.0009, recall: 0.3950, mrr: 0.1147, time: 941.6704943180084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2\n",
    "if not is_eval: #training\n",
    "        #Initialize the model\n",
    "        model = GRU4REC(input_size, hidden_size, output_size, final_act=final_act,\n",
    "                            num_layers=num_layers, use_cuda=cuda, batch_size=batch_size,\n",
    "                            dropout_input=dropout_input, dropout_hidden=dropout_hidden, embedding_dim=embedding_dim).to('cuda')\n",
    "        #weights initialization\n",
    "        #init_model(model)\n",
    "        #optimizer\n",
    "        optimizer = Optimizer(model.parameters(), optimizer_type=optimizer_type, lr=lr,\n",
    "                                  weight_decay=weight_decay, momentum=momentum, eps=eps)\n",
    "        #trainer class\n",
    "        trainer = Trainer(model, train_data=train_data, eval_data=valid_data, optim=optimizer,\n",
    "                              use_cuda=cuda, loss_func=loss_function, batch_size=batch_size,k_eval=k_eval)\n",
    "        print('#### START TRAINING....')\n",
    "        result1 = trainer.train(0, n_epochs - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-disability",
   "metadata": {},
   "source": [
    "Comme on s'y attendait, les temps d'exécution ont largement baissé. Les résultats obtenus ici avec ces tailles de batch (sur la BPR-MAX) ne sont pas très mauvais par rapport à ceux obtenus précédemment. On remarque que la performance du BPR-MAX s'améliore grandement ceci serait dû au fait qu'à défaut d'avoir des échantillons négatifs supplémentaires une taille de batch plus grande donne la possibilité au BPR-MAX d'avoir plus d'échantillons négatifs pertinents.\n",
    "\n",
    "Pour vérifier cette thèse, nous allons entrainé avec une taille de batch 1000 la BPR et observer ses métriques de performances. Si elle se dégradent on pourra conclure qu'en lieu et place d'utiliser des échantillons négatifs supplementaires sur les BPR-MAX et TOP1-MAX on pourrait tout simplement considérer une taille de batch plus grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "leading-orbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "loss_type = 'BPR'\n",
    "final_act = 'tanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "reliable-daisy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/31579 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### START TRAINING....\n",
      "Start Epoch # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████████████████████████████████▊                   | 23620/31579 [18:48<06:20, 20.93it/s]\n",
      " 71%|█████████████████████████████████████████████████████████▉                        | 41/58 [00:01<00:00, 32.68it/s]\n",
      "  0%|                                                                                        | 0/31579 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 0.0009, loss: 0.0009, recall: 0.3695, mrr: 0.1076, time: 1129.8343074321747\n",
      "Start Epoch # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████████████████████████████████▊                   | 23620/31579 [18:07<06:06, 21.72it/s]\n",
      " 71%|█████████████████████████████████████████████████████████▉                        | 41/58 [00:00<00:00, 41.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: 0.0009, loss: 0.0009, recall: 0.4051, mrr: 0.1204, time: 1088.5993971824646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2\n",
    "if not is_eval: #training\n",
    "        #Initialize the model\n",
    "        model = GRU4REC(input_size, hidden_size, output_size, final_act=final_act,\n",
    "                            num_layers=num_layers, use_cuda=cuda, batch_size=batch_size,\n",
    "                            dropout_input=dropout_input, dropout_hidden=dropout_hidden, embedding_dim=embedding_dim).to('cuda')\n",
    "        #weights initialization\n",
    "        #init_model(model)\n",
    "        #optimizer\n",
    "        optimizer = Optimizer(model.parameters(), optimizer_type=optimizer_type, lr=lr,\n",
    "                                  weight_decay=weight_decay, momentum=momentum, eps=eps)\n",
    "        #trainer class\n",
    "        trainer = Trainer(model, train_data=train_data, eval_data=valid_data, optim=optimizer,\n",
    "                              use_cuda=cuda, loss_func=loss_function, batch_size=batch_size,k_eval=k_eval)\n",
    "        print('#### START TRAINING....')\n",
    "        result1 = trainer.train(0, n_epochs - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-plenty",
   "metadata": {},
   "source": [
    "Ici nous challengeons les fonction d'activations terminales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "relative-tablet",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_act = 'softmax'\n",
    "batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "junior-raising",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/63158 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### START TRAINING....\n",
      "Start Epoch # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-e59e7dc184cb>:65: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logit = self.final_activation(self.h2o(output))\n",
      " 75%|████████████████████████████████████████████████████████▊                   | 47245/63158 [18:49<06:20, 41.82it/s]\n",
      " 75%|████████████████████████████████████████████████████████████▊                    | 87/116 [00:00<00:00, 89.10it/s]\n",
      "  0%|                                                                                        | 0/63158 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 0.0020, loss: 0.0020, recall: 0.0075, mrr: 0.0027, time: 1130.7661967277527\n",
      "Start Epoch # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████████████████████████████████▊                   | 47245/63158 [18:50<06:20, 41.79it/s]\n",
      " 75%|████████████████████████████████████████████████████████████▊                    | 87/116 [00:00<00:00, 89.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: 0.0020, loss: 0.0020, recall: 0.0203, mrr: 0.0183, time: 1131.4820203781128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2\n",
    "if not is_eval: #training\n",
    "        #Initialize the model\n",
    "        model = GRU4REC(input_size, hidden_size, output_size, final_act=final_act,\n",
    "                            num_layers=num_layers, use_cuda=cuda, batch_size=batch_size,\n",
    "                            dropout_input=dropout_input, dropout_hidden=dropout_hidden, embedding_dim=embedding_dim).to('cuda')\n",
    "        #weights initialization\n",
    "        #init_model(model)\n",
    "        #optimizer\n",
    "        optimizer = Optimizer(model.parameters(), optimizer_type=optimizer_type, lr=lr,\n",
    "                                  weight_decay=weight_decay, momentum=momentum, eps=eps)\n",
    "        #trainer class\n",
    "        trainer = Trainer(model, train_data=train_data, eval_data=valid_data, optim=optimizer,\n",
    "                              use_cuda=cuda, loss_func=loss_function, batch_size=batch_size,k_eval=k_eval)\n",
    "        print('#### START TRAINING....')\n",
    "        result6 = trainer.train(0, n_epochs - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "existing-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_act ='softmax_logit'\n",
    "loss_type = 'BPR-max'\n",
    "batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "lovely-clone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### START TRAINING....\n",
      "Start Epoch # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/63158 [00:00<?, ?it/s]<ipython-input-6-e59e7dc184cb>:65: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logit = self.final_activation(self.h2o(output))\n",
      " 75%|████████████████████████████████████████████████████████▊                   | 47245/63158 [18:57<06:23, 41.54it/s]\n",
      " 75%|████████████████████████████████████████████████████████████▊                    | 87/116 [00:01<00:00, 71.68it/s]\n",
      "  0%|                                                                                        | 0/63158 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 0.0026, loss: 0.0027, recall: 0.3179, mrr: 0.0868, time: 1138.622053861618\n",
      "Start Epoch # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████████████████████████████████▊                   | 47245/63158 [18:59<06:23, 41.47it/s]\n",
      " 75%|████████████████████████████████████████████████████████████▊                    | 87/116 [00:01<00:00, 74.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: 0.0026, loss: 0.0028, recall: 0.0985, mrr: 0.0220, time: 1140.4958963394165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2\n",
    "if not is_eval: #training\n",
    "        #Initialize the model\n",
    "        model = GRU4REC(input_size, hidden_size, output_size, final_act=final_act,\n",
    "                            num_layers=num_layers, use_cuda=cuda, batch_size=batch_size,\n",
    "                            dropout_input=dropout_input, dropout_hidden=dropout_hidden, embedding_dim=embedding_dim).to('cuda')\n",
    "        #weights initialization\n",
    "        #init_model(model)\n",
    "        #optimizer\n",
    "        optimizer = Optimizer(model.parameters(), optimizer_type=optimizer_type, lr=lr,\n",
    "                                  weight_decay=weight_decay, momentum=momentum, eps=eps)\n",
    "        #trainer class\n",
    "        trainer = Trainer(model, train_data=train_data, eval_data=valid_data, optim=optimizer,\n",
    "                              use_cuda=cuda, loss_func=loss_function, batch_size=batch_size,k_eval=k_eval)\n",
    "        print('#### START TRAINING....')\n",
    "        result6 = trainer.train(0, n_epochs - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-monster",
   "metadata": {},
   "source": [
    "On aboutit à la conclusion que la fonction TanH est la fonction optimale pour cette tâche."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-situation",
   "metadata": {},
   "source": [
    "## Conclusion <a class=\"anchor\" id=\"conclusion\"></a>\n",
    "\n",
    "\n",
    "En sommes tout au long de ce travail nous avons analysé l'article **Recurrent Neural Networks with Top-k Gains for Session-based Recommendations** et menés quelques expérimentations basées sur leur algorithme. Dans nos expérimentations nous avons abouti à conclusion que les nouvelles fonctions de pertes proposées par les auteurs étaient dépendantes du style d'échantillonnage. Ainsi en lieu et place d'utiliser des échantillons négatifs supplémentaires, on pourrait augmenter la taille du batch pour avoir de meilleurs résultats (la taille de batch conseillée avec ce jeu de données lorsqu'il y'a des échantillons négatifs supplémentaires varie entre 35 et 50, en abscence de ces échantillons négatifs supplémentaires on pourrait la faire passer à 2000 par exemple).\n",
    "\n",
    "Une amélioration qu'on pourrait aussi faire sur la TOP1-MAX est d'ajouter une coefficient de régularisation compris entre 0 et 1 sur la partie qui pénalise les échantillons négatifs. ceci va fortement baisser l'impact de ces derniers lors de l'apprentissage (surtout pour les moins pertinents)\n",
    "$$ L_s^{TOP1-MAX} = \\sum_{j=1}^{N_s}{s_j(\\sigma(\\hat r_{s,j} - \\hat r_{s,i}) + \\alpha \\sigma(\\hat r_{s,j}^2))} $$\n",
    "\n",
    "Une limite de notre travail est que nous avions pas pû tester l'algorithme des auteurs sur d'autres dataset. Nous en avons pas trouvé open source. Un travail future serait de tester cet algorithme sur de nouveuax jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-handle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
